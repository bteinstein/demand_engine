{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d16924",
   "metadata": {},
   "source": [
    "# NEW IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad8e08",
   "metadata": {},
   "source": [
    "# OLD IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 8,
   "id": "e444afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium routingpy openrouteservice geopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 157,
>>>>>>> e1188f7 (Modified Index .html)
=======
   "execution_count": 17,
>>>>>>> 25543de (Update map)
   "id": "14f01705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from src.get_data import get_data \n",
    "from src.get_data import get_connection\n",
    "from datetime import datetime, timedelta\n",
    "from src.routing.routing_optimizer import RouteOptimizer\n",
    "from src.routing.routing import get_valhalla_routes_info, plot_routes_on_map\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score \n",
    "import folium\n",
    "from src.map_viz.plot_cluster import create_enhanced_cluster_map\n",
    "import openrouteservice as ors\n",
    "import math\n",
    "import numpy as np\n",
    "import os \n",
    "from routingpy import Valhalla\n",
    "import shutil\n",
    "# client = ors.Client(key='5b3ce3597851110001cf62485a415b103df64104ad2680c9210ef936') \n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyodbc import Connection\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# from src.route_optimization import run_route_optimizer\n",
    "\n",
    "\n",
    "VALHALLA_BASE_URL = \"http://localhost:8002\" # Pointing to your self-hosted Valhalla\n",
    "VALHALLA_API_KEY = \"\" # No API key needed for your self-hosted instance\n",
    "\n",
    "\n",
    "CURRENT_DATE  = datetime.today().date() # + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4fa03",
   "metadata": {},
   "source": [
    "## **Utils**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e2ca2",
   "metadata": {},
   "source": [
    "#### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cc1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_invalid_coordinates_DEP(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces invalid Latitude (< -90 or > 90) and Longitude (< -180 or > 180) values with 0.0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'Latitude' and 'Longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with corrected coordinate values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df.loc[(df['Latitude'] < -90) | (df['Latitude'] > 90), 'Latitude'] = 0.0\n",
    "    df.loc[(df['Longitude'] < -180) | (df['Longitude'] > 180), 'Longitude'] = 0.0\n",
    "\n",
    "\n",
    "    ### Nigeria \n",
    "    # ADD NIGERIA FILTER HERE\n",
    "    return df\n",
    "\n",
    "def clean_invalid_coordinates(df: pd.DataFrame, offset_degrees: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces invalid Latitude (< -90 or > 90) and Longitude (< -180 or > 180) values with 0.0.\n",
    "    Also replaces coordinates outside Nigeria's approximate boundaries (with an optional offset) with 0.0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'Latitude' and 'Longitude' columns.\n",
    "        offset_degrees (float): Degrees to add/subtract from the strict Nigeria boundary\n",
    "                                to expand the bounding box. Default is 0.1 degrees.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with corrected coordinate values.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Global invalid coordinate ranges\n",
    "    df.loc[(df['Latitude'] < -90) | (df['Latitude'] > 90), 'Latitude'] = 0.0\n",
    "    df.loc[(df['Longitude'] < -180) | (df['Longitude'] > 180), 'Longitude'] = 0.0\n",
    "\n",
    "    ### Nigeria Boundary Filter ###\n",
    "    # Approximate decimal degree boundaries for Nigeria\n",
    "    STRICT_NIGERIA_MIN_LAT = 4.10\n",
    "    STRICT_NIGERIA_MAX_LAT = 13.90\n",
    "    STRICT_NIGERIA_MIN_LON = 2.60\n",
    "    STRICT_NIGERIA_MAX_LON = 14.70\n",
    "\n",
    "    # Apply offset to expand the bounding box\n",
    "    NIGERIA_MIN_LAT = STRICT_NIGERIA_MIN_LAT - offset_degrees\n",
    "    NIGERIA_MAX_LAT = STRICT_NIGERIA_MAX_LAT + offset_degrees\n",
    "    NIGERIA_MIN_LON = STRICT_NIGERIA_MIN_LON - offset_degrees\n",
    "    NIGERIA_MAX_LON = STRICT_NIGERIA_MAX_LON + offset_degrees\n",
    "\n",
    "    # Identify coordinates outside Nigeria's expanded bounding box\n",
    "    # Condition for rows outside Nigeria's latitude range\n",
    "    outside_nigeria_lat = (df['Latitude'] < NIGERIA_MIN_LAT) | \\\n",
    "                          (df['Latitude'] > NIGERIA_MAX_LAT)\n",
    "\n",
    "    # Condition for rows outside Nigeria's longitude range\n",
    "    outside_nigeria_lon = (df['Longitude'] < NIGERIA_MIN_LON) | \\\n",
    "                          (df['Longitude'] > NIGERIA_MAX_LON)\n",
    "\n",
    "    # Combine conditions: if EITHER latitude OR longitude is outside Nigeria's expanded box,\n",
    "    # then set BOTH Latitude and Longitude for that row to 0.0.\n",
    "    # We apply this only to coordinates that are already globally valid (i.e., not 0.0).\n",
    "    df.loc[\n",
    "        (df['Latitude'] != 0.0) &\n",
    "        (df['Longitude'] != 0.0) &\n",
    "        (outside_nigeria_lat | outside_nigeria_lon),\n",
    "        ['Latitude', 'Longitude']\n",
    "    ] = 0.0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "589e0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df_customer_sku_recommendation_raw, \n",
    "                      df_customer_dim_with_affinity_score_raw, \n",
    "                      df_stockpoint_dim_raw,\n",
    "                      df_customer_score,\n",
    "                      df_kyc_customer) :\n",
    "    \n",
    "    df_customer_sku_recommendation_raw['Stock_Point_ID'] = df_customer_sku_recommendation_raw['Stock_Point_ID'].astype(int)\n",
    "    df_customer_dim_with_affinity_score_raw['Stock_Point_ID'] = df_customer_dim_with_affinity_score_raw['Stock_Point_ID'].astype(int)\n",
    "    df_stockpoint_dim_raw['Stock_Point_ID'] = df_stockpoint_dim_raw['Stock_Point_ID'].astype(int)\n",
    "    df_customer_score = df_customer_score.rename(columns={'StockPointID':'Stock_Point_ID'})\n",
    "    df_customer_score['Stock_Point_ID'] = df_customer_score['Stock_Point_ID'].astype(int)\n",
    "\n",
    "\n",
    "    # ----------------- CUSTOMER DIM TABLE \n",
    "    col_sel_affinity = ['Region', 'Stock_Point_ID', 'CustomerID']\n",
    "\n",
    "    col_sel_kyc = ['CustomerID', 'ContactName', 'BusinessName', 'CustomerModeName',\n",
    "        'CustomerRef', 'ContactPhone', 'CustomerType', 'FullAddress', \n",
    "        'StateName', 'CityName', 'TownName', 'Latitude','Longitude', \n",
    "        'DistanceVarianceInMeter', 'IsLocationSubmitted',\n",
    "        'IsLocationCaptured', 'IsLocationVerified','CustomerStatus',\n",
    "        'RejectReason',  'KYC_Capture_Status',  'lastDelvDate', \n",
    "        # 'hasPOS','hasVAS', 'hasBNPL', 'lastDelvDate', \n",
    "        'isActive']\n",
    "\n",
    "    col_sel_score = ['Stock_Point_ID', 'CustomerID', 'composite_customer_score',\n",
    "        'percentile_rank', 'active_months_pct', 'avg_orders_per_active_month',\n",
    "        'avg_qty_per_month', 'avg_revenue_per_month', 'days_since_last_order']\n",
    "\n",
    "    df_master_customer_dim = (\n",
    "                df_customer_dim_with_affinity_score_raw[col_sel_affinity]\n",
    "                .merge(df_kyc_customer[col_sel_kyc], how='inner', on=['CustomerID'])\n",
    "                .merge(df_customer_score[col_sel_score], how='left', on=['Stock_Point_ID', 'CustomerID'])\n",
    "                .rename(columns = {'CityName':'LGA',\n",
    "                                'TownName':'LCDA'\n",
    "                                })\n",
    "\n",
    "            )\n",
    "\n",
    "    # Change CustomerPurchaseRecency from lastDelvDate to days since last order (order creation date)\n",
    "    df_master_customer_dim['CustomerPurchaseRecency'] =  df_master_customer_dim['days_since_last_order']\n",
    "    # df_master_customer_dim['CustomerPurchaseRecency'] =  df_master_customer_dim['lastDelvDate'].apply(lambda x: (datetime.now() - x).days)\n",
    "    df_master_customer_dim['CustomerPurchaseRecency'] = df_master_customer_dim['CustomerPurchaseRecency'].fillna(max(df_master_customer_dim['CustomerPurchaseRecency']))\n",
    "    df_master_customer_dim['KYC_Capture_Status'] = df_master_customer_dim['KYC_Capture_Status'].apply(lambda x: 'Yes' if x == 1 else 'No')\n",
    "\n",
    "    # Add to Score\n",
    "    # Fix Missing value -------------------------------------------\n",
    "    for col in ['BusinessName', 'CustomerModeName', 'FullAddress', 'LGA', 'LCDA']:\n",
    "        df_master_customer_dim[col] = df_master_customer_dim[col].fillna('')\n",
    "\n",
    "    for col in ['Latitude',  'Longitude', 'composite_customer_score',  \n",
    "                'percentile_rank',  'active_months_pct', 'avg_orders_per_active_month',  \n",
    "                'avg_qty_per_month',  'avg_revenue_per_month'\n",
    "                ]:\n",
    "        df_master_customer_dim[col] = pd.to_numeric(df_master_customer_dim[col], errors='coerce').fillna(0) \n",
    "\n",
    "    df_master_customer_dim = clean_invalid_coordinates(df_master_customer_dim)\n",
    "    \n",
    "    # Add to Score \n",
    "    # Boost composite score and percentile rank for customers with completed KYC\n",
    "    mask_kyc = df_master_customer_dim['KYC_Capture_Status'] == 'Yes'\n",
    "\n",
    "    df_master_customer_dim.loc[mask_kyc, 'composite_customer_score'] += 5\n",
    "    df_master_customer_dim.loc[mask_kyc, 'percentile_rank'] += 0.1 \n",
    "\n",
    "    # ----------------- RECOMMENDATION\n",
    "    col2 = ['EstimatedQuantity', 'CustomerSKUscore', 'CustomerSKUscoreStandardize', 'CustomerSKUscoreRank']\n",
    "    for col in col2: \n",
    "        df_customer_sku_recommendation_raw[col] = pd.to_numeric(df_customer_sku_recommendation_raw[col], errors='coerce')\n",
    "\n",
    "    df_customer_sku_recommendation_raw['LastDeliveredDate'] = pd.to_datetime(df_customer_sku_recommendation_raw['LastDeliveredDate'])\n",
    "    # Get today's date\n",
    "    today = pd.Timestamp.today()\n",
    "\n",
    "    df_customer_sku_recommendation_raw['Recency'] = df_customer_sku_recommendation_raw['LastDeliveredDate'].apply(lambda x: (datetime.now() - x).days)\n",
    "    df_customer_sku_recommendation_raw['Recency'] = df_customer_sku_recommendation_raw['Recency'].fillna(max(df_customer_sku_recommendation_raw['Recency']))\n",
    "    \n",
    "    # ----------------- STOCKPOINT\n",
    "    df_stockpoint_dim_raw.rename(columns={'lattitude':'Latitude', 'longitude':'Longitude'}, inplace=True) \n",
    "    col3 = ['Latitude', 'Longitude']\n",
    "    for col in col3: \n",
    "        df_stockpoint_dim_raw[col] = pd.to_numeric(df_stockpoint_dim_raw[col], errors='coerce').fillna(0)    \n",
    "\n",
    "    # Replace invalid latitude values with NaN\n",
    "    df_stockpoint_dim_raw = clean_invalid_coordinates(df_stockpoint_dim_raw)   \n",
    "    \n",
    "\n",
    "    return df_customer_sku_recommendation_raw, df_master_customer_dim, df_stockpoint_dim_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50d39f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filter(df_customer_sku_recommendation, df_master_customer_dim, df_stockpoint_dim,\n",
    "                stockpoint_id,  sku_recency = 7, customer_recency = 90, number_recommendation = 5,\n",
    "                estimate_qty_scale_factor = .90, max_estimated_qty = 5, exclude_recency_customer = 4):\n",
    "    \n",
    "    df_customer_sku_recommendation = df_customer_sku_recommendation.copy().query(f'Stock_Point_ID == {stockpoint_id}')\n",
    "    # Filter Recommendation\n",
    "    df_customer_sku_recommendation = df_customer_sku_recommendation[df_customer_sku_recommendation['ProductTag'] != 'Standard-Inactive']\n",
    "    df_customer_sku_recommendation = df_customer_sku_recommendation[df_customer_sku_recommendation['Medium'] != 'Never Purchased']\n",
    "\n",
    "    # Filter customer base\n",
    "    df_master_customer_dim['valid_for_push'] = np.where(\n",
    "                                                    #  df_master_customer_dim['KYC_Capture_Status'] == 'Yes'   \n",
    "                                                    (\n",
    "                                                        (df_master_customer_dim['IsLocationCaptured'] == 'Yes') |\n",
    "                                                        (df_master_customer_dim['DistanceVarianceInMeter'] <= 150.0) |\n",
    "                                                        (df_master_customer_dim['KYC_Capture_Status'] == 'Yes') |\n",
    "                                                        (df_master_customer_dim['CustomerPurchaseRecency'] <= customer_recency)\n",
    "                                                    )\n",
    "                                                    ,1,0\n",
    "                                                )\n",
    "    # df_master_customer_dim = df_master_customer_dim[df_master_customer_dim['CustomerPurchaseRecency'] <= customer_recency]\n",
    "    df_master_customer_dim = df_master_customer_dim.query('valid_for_push == 1')  \n",
    "    # Exclude Customer with recent purchase of any SKU\n",
    "    df_master_customer_dim = df_master_customer_dim.query(f'CustomerPurchaseRecency > {exclude_recency_customer}')\n",
    "    # Customer with valid Location Coordination\n",
    "    df_master_customer_dim = df_master_customer_dim.query('Latitude != 0').reset_index(drop=True)\n",
    "    \n",
    "    # # Clipping Max Estimated Quantity to 10 qty\n",
    "    df_customer_sku_recommendation['EstimatedQuantity_bck'] = df_customer_sku_recommendation['EstimatedQuantity']\n",
    "    df_customer_sku_recommendation['EstimatedQuantity'] = df_customer_sku_recommendation['EstimatedQuantity'].apply(lambda x: max_estimated_qty if int((x*estimate_qty_scale_factor)) > max_estimated_qty else int((x*estimate_qty_scale_factor)) )\n",
    "\n",
    "\n",
    "    # Select top 10 SKU by SKURank per customer\n",
    "    df_customer_sku_recommendation = (\n",
    "        df_customer_sku_recommendation\n",
    "        .query('EstimatedQuantity > 1')\n",
    "        .sort_values(['CustomerID','CustomerSKUscoreRank'])\n",
    "        .groupby('CustomerID', group_keys=False)\n",
    "        .head(number_recommendation)\n",
    "        .reset_index(drop=True) \n",
    "    )\n",
    "\n",
    "    df_customer_sku_recommendation_ = df_master_customer_dim.merge(df_customer_sku_recommendation, how='inner', on = ['CustomerID','Stock_Point_ID'])  \n",
    "\n",
    "    df_stockpoint_dim = df_stockpoint_dim.query(f'Stock_Point_ID == {stockpoint_id}').reset_index(drop=True) \n",
    "    \n",
    "\n",
    "    df_customer_dim = df_master_customer_dim.merge(df_customer_sku_recommendation_['CustomerID'].drop_duplicates(), how='inner', on = 'CustomerID')\n",
    "    # df_customer_dim = df_customer_dim.merge(df_customer_dim_with_affinity_score[sel_cols], how='inner', on = 'CustomerID').reset_index(drop = True) \n",
    "    \n",
    "    print(f'Total Quantity before filter: {df_customer_sku_recommendation.query(f\"Stock_Point_ID == {stockpoint_id}\").EstimatedQuantity.sum():,}')\n",
    "    print(f'Total Quantity: {df_customer_sku_recommendation_.EstimatedQuantity.sum():,}')\n",
    "    print(f'Total Number of Customers before filter: {df_customer_sku_recommendation.query(f\"Stock_Point_ID == {stockpoint_id}\").CustomerID.nunique():,}')\n",
    "    print(f'Total Number of Customers: {df_customer_dim.CustomerID.nunique():,}')\n",
    "\n",
    " \n",
    "    return df_customer_sku_recommendation_, df_customer_dim,   df_stockpoint_dim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a26754f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(\n",
    "        selected_trip,\n",
    "        all_push_recommendation,\n",
    "        cluster_summary,\n",
    "        stock_point_name\n",
    "    ): \n",
    "    dir_path = f'./recommendation_output/excel_docs/{CURRENT_DATE}'\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    file_path = f'{dir_path}/{stock_point_name}_{CURRENT_DATE}.xlsx'\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        selected_trip.to_excel(writer, sheet_name='Selected Trip', index=False)\n",
    "        all_push_recommendation.to_excel(writer, sheet_name='All Recommendation', index=False)\n",
    "        cluster_summary.to_excel(writer, sheet_name='Recommendation Cluster Summary', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939127f3",
   "metadata": {},
   "source": [
    "#### Map-Utils"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 162,
>>>>>>> e1188f7 (Modified Index .html)
=======
   "execution_count": null,
>>>>>>> 25543de (Update map)
   "id": "7525cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_and_save(df_routes, \n",
    "                 df_stockpoint = None,   \n",
    "                 filename=None,\n",
    "                 cluster_col='cluster',\n",
    "                 popup_cols=['CustomerID', 'LGA', 'LCDA'],\n",
<<<<<<< HEAD
    "                 tooltip_cols=['LGA', 'LCDA'],\n",
    "                 depot_marker_size=10\n",
    "                 ):\n",
=======
    "                 tooltip_cols=['LGA', 'LCDA']):\n",
>>>>>>> 25543de (Update map)
    "    \n",
    "    map_clusters = create_enhanced_cluster_map(\n",
    "        df_routes,\n",
    "        popup_cols=popup_cols,\n",
    "        tooltip_cols=tooltip_cols, \n",
    "        cluster_col = cluster_col,\n",
    "        zoom_start=10, \n",
    "        radius=8\n",
    "    )\n",
    "    \n",
    "    if not df_stockpoint.empty:\n",
    "        depot_location = [df_stockpoint.Latitude[0], df_stockpoint.Longitude[0]]\n",
    "        depot_name = df_stockpoint.Stock_point_Name[0]\n",
    "        map_clusters = map_clusters.add_child(folium.Marker(location=depot_location, \n",
    "                                size = depot_marker_size, \n",
    "                                tooltip=depot_name, \n",
    "                                icon=folium.Icon(color=\"green\", \n",
    "                                icon=\"home\")))  \n",
    "    if filename:\n",
    "        map_clusters.save(filename)\n",
    "    return map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2646ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_map_all_clusters(df_routes, df_stockpoint, stock_point_id):\n",
    "    cluster_map_dir_path = f'./recommendation_output/cluster_map/{CURRENT_DATE}'\n",
    "    os.makedirs(cluster_map_dir_path, exist_ok=True)\n",
    "    \n",
    "    # copy default index.html of recommendation_output dir to to main_dir\n",
    "    try:\n",
    "        shutil.copy('./recommendation_output/cluster_map/index.html', f'{cluster_map_dir_path}/index.html')\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Error copying index.html: {e}') \n",
    "        \n",
    "        \n",
    "    cluster_map_path = f'{cluster_map_dir_path}/{stock_point_id}.html' \n",
    "    \n",
    "    _ = vis_and_save(df_routes= (df_routes\n",
    "                                .rename(columns={'cluster':'cluster_bck'})\n",
    "                                .rename(columns={'TripNumber':'cluster'}) ), \n",
    "                                df_stockpoint=df_stockpoint, \n",
    "                                filename=cluster_map_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff9846",
   "metadata": {},
   "source": [
    "#### Cluster Summary Route-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee878c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unsupervised_clustering(df):\n",
    "    # Usage:\n",
    "    X = df[['Latitude', 'Longitude']].values\n",
    "    labels = df['cluster'].values\n",
    "    scores = {\n",
    "        \"Silhouette Score\":  silhouette_score(X, labels).round(2),\n",
    "        \"Davies-Bouldin Index\": davies_bouldin_score(X, labels).round(2),\n",
    "        \"Calinski-Harabasz Score\": calinski_harabasz_score(X, labels).round(2)\n",
    "    }\n",
    "\n",
    "    for key in scores:\n",
    "        print(f\"{key}: {scores[key]}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b65cbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stockpoint_dict(df_selected_trip, df_stockpoint_dim):\n",
    "    \"\"\"\n",
    "    Create a dictionary structure with stock point information and associated trips.\n",
    "    \n",
    "    Parameters:\n",
    "    df_selected_trip: DataFrame with columns ['StockPointID', 'StockPointName', 'TripID', 'CustomerID', 'Latitude', 'Longitude', 'EstimatedQuantity']\n",
    "    df_stockpoint_dim: DataFrame with columns ['Stock_Point_ID', 'Stock_point_Name', 'Latitude', 'Longitude']\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with stock point information and trips\n",
    "    \"\"\"\n",
    "    if df_selected_trip.empty:\n",
    "        logger.info('Dataframe is empty')\n",
    "        return {}\n",
    "    \n",
    "    # Group by StockPointID to handle each stock point\n",
    "    stockpoint_groups = df_selected_trip.groupby('StockPointID')\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for stock_point_id, group in stockpoint_groups:\n",
    "        # Get stock point information from df_stockpoint_dim\n",
    "        stock_point_info = df_stockpoint_dim[df_stockpoint_dim['Stock_Point_ID'] == stock_point_id]\n",
    "        \n",
    "        if stock_point_info.empty:\n",
    "            # If stock point not found in dimension table, use info from selected_trip\n",
    "            stock_point_name = group['StockPointName'].iloc[0]\n",
    "            # Note: We'll need to get coordinates from somewhere since customer coordinates \n",
    "            # in df_selected_trip are for destinations, not stock points\n",
    "            stock_point_coord = [0, 0]  # Placeholder - you may need to adjust this\n",
    "        else:\n",
    "            stock_point_name = stock_point_info['Stock_point_Name'].iloc[0]\n",
    "            stock_point_coord = [\n",
    "                stock_point_info['Longitude'].iloc[0], \n",
    "                stock_point_info['Latitude'].iloc[0]\n",
    "            ]\n",
    "        \n",
    "        # Group by TripID to organize trips\n",
    "        trip_groups = group.groupby('TripID')\n",
    "        trips = []\n",
    "        \n",
    "        for trip_id, trip_group in trip_groups:\n",
    "            # Create destinations list for this trip\n",
    "            destinations = []\n",
    "            for _, row in trip_group.iterrows():\n",
    "                destination = {\n",
    "                    'CustomerID': row['CustomerID'],\n",
    "                    'Coordinate': [row['Longitude'], row['Latitude']]\n",
    "                }\n",
    "                destinations.append(destination)\n",
    "            \n",
    "            # Create trip dictionary\n",
    "            trip_dict = {\n",
    "                'TripID': trip_id,\n",
    "                'Destinations': destinations\n",
    "            }\n",
    "            trips.append(trip_dict)\n",
    "        \n",
    "        # Create the final dictionary structure for this stock point\n",
    "        result[stock_point_id] = {\n",
    "            'StockPointName': stock_point_name,\n",
    "            'StockPointID': stock_point_id,\n",
    "            'StockPointCoord': stock_point_coord,\n",
    "            'Trips': trips\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Alternative version if you want a single dictionary (assuming only one stock point)\n",
    "def create_single_stockpoint_dict(df_selected_trip, df_stockpoint_dim):\n",
    "    \"\"\"\n",
    "    Create a single dictionary structure for one stock point.\n",
    "    \n",
    "    Parameters:\n",
    "    df_selected_trip: DataFrame with trip data for one stock point\n",
    "    df_stockpoint_dim: DataFrame with stock point dimension data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Single dictionary with stock point information and trips\n",
    "    \"\"\"\n",
    "    if df_selected_trip.empty:\n",
    "        logger.info('Dataframe is empty')\n",
    "        return {}\n",
    "    \n",
    "    # Get the stock point ID (assuming all rows have the same stock point)\n",
    "    stock_point_id = df_selected_trip['StockPointID'].iloc[0]\n",
    "    \n",
    "    # Get stock point information from df_stockpoint_dim\n",
    "    stock_point_info = df_stockpoint_dim[df_stockpoint_dim['Stock_Point_ID'] == stock_point_id]\n",
    "    \n",
    "    if stock_point_info.empty:\n",
    "        stock_point_name = df_selected_trip['StockPointName'].iloc[0]\n",
    "        stock_point_coord = [0, 0]  # Placeholder\n",
    "    else:\n",
    "        stock_point_name = stock_point_info['Stock_point_Name'].iloc[0] \n",
    "        stock_point_coord = [\n",
    "            stock_point_info['Longitude'].iloc[0], \n",
    "            stock_point_info['Latitude'].iloc[0]\n",
    "        ]\n",
    "    \n",
    "    # Group by TripID\n",
    "    trip_groups = df_selected_trip.groupby('TripID')\n",
    "    trips = []\n",
    "    \n",
    "    for trip_id, trip_group in trip_groups:\n",
    "        destinations = []\n",
    "        for _, row in trip_group.iterrows():\n",
    "            destination = {\n",
    "                'CustomerID': row['CustomerID'],\n",
    "                'Coordinate': [row['Longitude'], row['Latitude']]\n",
    "            }\n",
    "            destinations.append(destination)\n",
    "        \n",
    "        trip_dict = {\n",
    "            'TripID': trip_id,\n",
    "            'Destinations': destinations\n",
    "        }\n",
    "        trips.append(trip_dict)\n",
    "    \n",
    "    # Return the final dictionary\n",
    "    return {\n",
    "        'StockPointName': stock_point_name,\n",
    "        'StockPointID': stock_point_id,\n",
    "        'StockPointCoord': stock_point_coord,\n",
    "        'Trips': trips\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# \"\"\"\n",
    "# # For multiple stock points:\n",
    "# result_dict = create_stockpoint_dict(df_selected_trip, df_stockpoint_dim)\n",
    "\n",
    "# # For a single stock point:\n",
    "# single_result = create_single_stockpoint_dict(df_selected_trip, df_stockpoint_dim)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4c7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_route(df_selected_trip, df_stockpoint_dim):\n",
    "    # Path \n",
    "    main_dir = f'./recommendation_output/selected_trip_map/{CURRENT_DATE}' \n",
    "    os.makedirs(f'{main_dir}', exist_ok=True)\n",
    "    # copy default index.html of recommendation_output dir to to main_dir\n",
    "    try:\n",
    "        shutil.copy('./recommendation_output/selected_trip_map/index.html', f'{main_dir}/index.html')\n",
    "    except Exception as e:\n",
    "        logger.warn(f'Error copying index.html: {e}') \n",
    "\n",
    "    trip_dict = create_single_stockpoint_dict(df_selected_trip, df_stockpoint_dim) \n",
    "\n",
    "    if trip_dict == {}:\n",
    "        logger.info('Trip Data is empty')\n",
    "    else:\n",
    "        try:\n",
    "            StockPointID = trip_dict['StockPointID']\n",
    "            output_filename = f'{main_dir}/{StockPointID}.html'\n",
    "            # Step 1: Get route information for all trips\n",
    "            calculated_routes_info = get_valhalla_routes_info(trip_dict)\n",
    "\n",
    "            # Step 2: Plot all routes on a map\n",
    "            plot_routes_on_map(trip_data=trip_dict, routes_info=calculated_routes_info, output_filename = output_filename)\n",
    "        except Exception as e:\n",
    "            logger.warn(f'Some vital error occured while creating route {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dce62501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_route_optimizer(df_clustering, sel_cluster_tuple, df_stockpoint, \n",
    "                        stock_point_name,\n",
    "                        sel_total_customer_count, capacity_size = 20):\n",
    "     # ---- SETUP CLIENT\n",
    "     try:\n",
    "          client = Valhalla(base_url=VALHALLA_BASE_URL)\n",
    "          if VALHALLA_API_KEY:\n",
    "               client = Valhalla(base_url=VALHALLA_BASE_URL, api_key=VALHALLA_API_KEY)\n",
    "          \n",
    "          logger.info('Setting up routing client via LOCAL host Valhalla')\n",
    "     except Exception as e:\n",
    "          logger.warning('Setting up routing client via ORS')\n",
    "          client = ors.Client(key=os.getenv('ORS_KEY')) \n",
    "\n",
    "     # Select cluster 37\n",
    "     df_sel_clust = df_clustering.query(f'cluster in {sel_cluster_tuple}').query('Latitude > 0')\n",
    "\n",
    "     # Ensure coordinates are in [longitude, latitude] for ORS\n",
    "     coords = [[lon, lat] for lat, lon in zip(df_sel_clust.Latitude, df_sel_clust.Longitude)]\n",
    "     # Print number of jobs\n",
    "     print(\"Number of customer locations:\", len(coords))\n",
    "     # Convert depot_location to ORS format\n",
    "     # Assuming depot_location is [lat, lon], flip to [lon, lat]\n",
    "     vehicle_start = [df_stockpoint.Longitude[0], df_stockpoint.Latitude[0]]\n",
    "     num_vehicles = math.floor(sel_total_customer_count / capacity_size)\n",
    "     vehicles = [\n",
    "          ors.optimization.Vehicle(\n",
    "               id=i,\n",
    "               profile='driving-car',\n",
    "               start=vehicle_start,\n",
    "               end=vehicle_start,\n",
    "               capacity=[capacity_size]\n",
    "          ) for i in range(num_vehicles)\n",
    "     ]\n",
    "\n",
    "     # Define jobs (each customer gets amount=[1])\n",
    "     jobs = [ors.optimization.Job(id=index, location=coord, amount=[1]) for index, coord in enumerate(coords)]\n",
    "\n",
    "     # Call ORS optimization API\n",
    "     optimized = client.optimization(jobs=jobs, vehicles=vehicles, geometry=True)\n",
    "\n",
    "     #     ------ MAP\n",
    "     depot_location = [df_stockpoint.Latitude[0], df_stockpoint.Longitude[0]]\n",
    "     depot_name = df_stockpoint.Stock_point_Name[0]\n",
    "\n",
    "     map_clusters_route = create_enhanced_cluster_map(\n",
    "     df_sel_clust,\n",
    "     popup_cols=['CustomerID', 'LGA', 'LCDA'],\n",
    "     tooltip_cols=['LGA', 'LCDA'], \n",
    "     zoom_start=10, \n",
    "     radius=10\n",
    "     ).add_child(folium.Marker(location=depot_location, \n",
    "                         size = 10, \n",
    "                         tooltip=depot_name, \n",
    "                         icon=folium.Icon(color=\"green\", \n",
    "                         icon=\"home\")))\n",
    "\n",
    "     # line_colors = ['green', 'orange', 'blue', 'yellow']\n",
    "     separable_colors = [\n",
    "          \"#1f77b4\",  # blue\n",
    "          \"#ff7f0e\",  # orange\n",
    "          \"#2ca02c\",  # green\n",
    "          \"#d62728\",  # red\n",
    "          \"#9467bd\",  # purple\n",
    "          \"#8c564b\",  # brown\n",
    "          \"#e377c2\",  # pink\n",
    "          \"#7f7f7f\",  # gray\n",
    "          \"#bcbd22\",  # yellow-green\n",
    "          \"#17becf\",  # cyan\n",
    "          \"#aec7e8\",  # light blue\n",
    "          \"#ffbb78\",  # light orange\n",
    "          ]\n",
    "\n",
    "     line_colors = separable_colors[0:num_vehicles] #['green', 'orange', 'blue', 'yellow']\n",
    "     for route in optimized['routes']:\n",
    "          folium.PolyLine(locations=[list(reversed(coords)) for coords in ors.convert.decode_polyline(route['geometry'])['coordinates']], color=line_colors[route['vehicle']]).add_to(map_clusters_route)\n",
    "\n",
    "     #\n",
    "     selected_trip_map_path = f'./recommendation_output/selected_trip_map/{stock_point_name}_{CURRENT_DATE}.html' \n",
    "     map_clusters_route.save(selected_trip_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b107eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_trip_route(df_sku_rec, \n",
    "                       df_customer_dim, \n",
    "                       df_stockpoint,\n",
    "                       stock_point_id,\n",
    "                       max_customers_per_route, \n",
    "                       max_volume_per_route,\n",
    "                       max_distance_km, \n",
    "                       clustering_method='divisive',\n",
    "                       skip_route_optimization = False):\n",
    "     \n",
    "\n",
    "    optimizer = RouteOptimizer(\n",
    "        max_customers_per_route=max_customers_per_route,\n",
    "        max_volume_per_route=max_volume_per_route,\n",
    "        max_distance_km = max_distance_km\n",
    "    )\n",
    "\n",
    "    optimizer.load_data(df_sku_rec, df_customer_dim, df_stockpoint)\n",
    "    print(\"✓ Route optimizer initialized\")\n",
    "\n",
    "    # STEP 3: Generate Routes for Stock Point 1647113\n",
    "    print(\"\\n3. Generating Optimized Routes...\")\n",
    "    print(\"-\" * 40) \n",
    "\n",
    "    stock_point = df_stockpoint[df_stockpoint['Stock_Point_ID'] == stock_point_id].reset_index(drop = True)\n",
    "    \n",
    "    stock_point_coords = (stock_point['Latitude'], stock_point['Longitude'])\n",
    "        \n",
    "    clustering_customers_df = optimizer.filter_customers_for_stockpoint(stock_point_id)\n",
    "\n",
    "    df_clustering, n_clusters = optimizer.create_geographic_clusters(clustering_customers_df, \n",
    "                                                                     clustering_method = clustering_method)\n",
    "\n",
    "    if skip_route_optimization == True:\n",
    "        routes = optimizer.generate_multi_trip_routes(stock_point_id, \n",
    "                                                    max_trips=5, \n",
    "                                                    clustering_method=clustering_method)\n",
    "        df_routes = pd.DataFrame(routes)\n",
    "    else:\n",
    "        df_routes = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    # STEP 4: Analyze Results\n",
    "    print(\"\\n4. Route Analysis & Results...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    push_recommendation = df_sku_rec.merge(df_clustering[['Stock_Point_ID','CustomerID', 'cluster']], \n",
    "                                           how='inner', on =['Stock_Point_ID','CustomerID'] )\n",
    "    \n",
    "    ### Cluster Evaluation\n",
    "    try:\n",
    "        evaluate_unsupervised_clustering(df_clustering)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return push_recommendation, df_clustering, df_routes, stock_point_coords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bc8a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_summary_and_selection(push_recommendation,\n",
    "                                  sel_trip_cluster,\n",
    "                                  min_ncust_per_cluster = 4\n",
    "                                  ):\n",
    "    ### Cluster Summary \n",
    "    cluster_summary = (\n",
    "        push_recommendation\n",
    "        .groupby('cluster').agg(\n",
    "            LGA_list = ('LGA', lambda x: x.unique().tolist()),\n",
    "            LCDA_List = ('LCDA', lambda x: x.unique().tolist()),\n",
    "            ncustomer = ('CustomerID','nunique'),\n",
    "            totalQty = ('EstimatedQuantity','sum'), \n",
    "            avg_customer_score = ('composite_customer_score','mean'),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(['avg_customer_score','ncustomer', 'totalQty'], \n",
    "                     ascending=[False, False, False])\n",
    "        )\n",
    "\n",
    "    ### Select Trip   \n",
    "    df_high_value_cluster_summary = (\n",
    "            cluster_summary\n",
    "            .query(f'ncustomer >= {min_ncust_per_cluster}')\n",
    "            .head(max(10, sel_trip_cluster))\n",
    "            .reset_index(drop = True)\n",
    "        )\n",
    "    sel_cluster_tuple = df_high_value_cluster_summary.cluster[0:sel_trip_cluster].to_list()\n",
    "    sel_total_customer_count = df_high_value_cluster_summary.head(sel_trip_cluster).ncustomer.sum()\n",
    "    print(f'''Select ClusterIDs: {sel_cluster_tuple}''')\n",
    "    print(f'''Total Number of Customers: {sel_total_customer_count}''')\n",
    "    print(df_high_value_cluster_summary.head(sel_trip_cluster))\n",
    "\n",
    "    return cluster_summary, df_high_value_cluster_summary, sel_cluster_tuple, sel_total_customer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "171a5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_selected_trip(push_recommendation, \n",
    "                       cluster_summary,\n",
    "                       df_master_customer_dim,  \n",
    "                       df_stockpoint,\n",
    "                       sel_cluster_tuple):\n",
    "    \n",
    "        \n",
    "\n",
    "    sel_columns = ['Stock_Point_ID', \n",
    "                'StateName', # 'Region', \n",
    "                'Latitude', 'Longitude', 'LGA', 'LCDA', 'cluster', \n",
    "                'CustomerID', 'SKUID', 'ProductName', 'Output',\n",
    "                'LastDeliveredDate', 'Recency', 'InventoryCheck', 'ProductTag', 'Medium',\n",
    "                'EstimatedQuantity', \n",
    "                # 'CustomerSKUscoreRank'\n",
    "                ]\n",
    "\n",
    "    sel_cols_cust= ['Stock_Point_ID', 'CustomerID', 'ContactName',  'CustomerModeName',   'ContactPhone', 'FullAddress', \n",
    "                    'composite_customer_score', 'percentile_rank',  'KYC_Capture_Status', 'CustomerPurchaseRecency']\n",
    "\n",
    "    final_cols = ['Stock_Point_ID', 'Stock_point_Name', 'TripID', 'LGA_list', 'LCDA_List', \n",
    "                  'ncustomer', 'totalQty','avg_customer_score', 'CustomerID', 'ContactName',  \n",
    "                  'CustomerModeName',   'ContactPhone', 'FullAddress', 'Latitude',\n",
    "                  'Longitude', 'LGA', 'LCDA', 'composite_customer_score', #, 'percentile_rank',  \n",
    "                  'KYC_Capture_Status', 'SKUID', 'ProductName', #'Output', 'LastDeliveredDate', \n",
    "                  'Recency','CustomerPurchaseRecency', 'InventoryCheck', 'ProductTag', 'Medium', 'EstimatedQuantity',\n",
    "                ]\n",
    "    \n",
    "    def _merge_select(df):\n",
    "        modified_df = (\n",
    "                        df[sel_columns]\n",
    "                        .merge(cluster_summary, how='left', on = 'cluster' )\n",
    "                        .merge(df_master_customer_dim[sel_cols_cust], how='left', on = ['Stock_Point_ID', 'CustomerID'])\n",
    "                        .merge(df_stockpoint[['Stock_Point_ID', 'Stock_point_Name']], how='left', on = ['Stock_Point_ID'])\n",
    "                        .rename(columns={'cluster':'TripID'})\n",
    "                        [final_cols]\n",
    "                        .rename(columns = {\n",
    "                                           'Stock_point_Name': 'StockPointName'\n",
    "                                           ,'Stock_Point_ID': 'StockPointID'\n",
    "                                           ,'ncustomer': 'TotalCustonerCount'\n",
    "                                           ,'totalQty': 'TripTotalQuantity'\n",
    "                                           ,'avg_customer_score': 'TripAvgCustomerScore'\n",
    "                                           ,'LastDeliveredDate': 'CustomerLastDeliveredDate'\n",
    "                                           ,'Medium': 'RecommendationType'\n",
    "                                           ,'Recency': 'SKUDaysSinceLastBuy'\n",
    "                                           ,'CustomerPurchaseRecency': 'CustomerDaysSinceLastBuy'\n",
    "                                           ,'composite_customer_score': 'CustomerScore'\n",
    "                                           ,'KYC_Capture_Status': 'kycCaptureStatus'\n",
    "                                           ,'LGA_list': 'ClusterLGAs'\n",
    "                                           ,'LCDA_List': 'ClusterLCDAs'\n",
    "                                           })\n",
    "                        )\n",
    "        return modified_df\n",
    "\n",
    "    df_selected_trip = push_recommendation[push_recommendation['cluster'].isin(sel_cluster_tuple)]\n",
    "    selected_push_recommendation_trip = _merge_select(df_selected_trip)\n",
    "    all_push_recommendation =  _merge_select(push_recommendation)\n",
    "    all_push_recommendation['isTripSelected'] = np.where(all_push_recommendation['TripID'].isin(sel_cluster_tuple) ,\n",
    "                                                    'Yes',\n",
    "                                                    'No'\n",
    "                                                )\n",
    "    \n",
    "\n",
    "    return selected_push_recommendation_trip, all_push_recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d067f",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 171,
>>>>>>> e1188f7 (Modified Index .html)
=======
   "execution_count": 31,
>>>>>>> 25543de (Update map)
   "id": "bd91edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_push_recommendation(df_customer_sku_recommendation, \n",
    "                            df_master_customer_dim, \n",
    "                            df_stockpoint_dim, \n",
    "                            stock_point_id,\n",
    "                            stock_point_name,\n",
    "                            sku_recency = 7, \n",
    "                            customer_recency = 60, number_recommendation = 5, \n",
    "                            estimate_qty_scale_factor = 1, max_estimated_qty = 5, \n",
    "                            exclude_recency_customer = 4,\n",
    "                            max_customers_per_route=20,\n",
    "                            max_volume_per_route=300,\n",
    "                            max_distance_km = 40,\n",
    "                            sel_trip_cluster = 5,\n",
    "                            min_ncust_per_cluster = 5,\n",
    "                            clustering_method = 'divisive',\n",
    "                            skip_route_optimization = False):\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating complete route optimization workflow\n",
    "    \"\"\" \n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ROUTE OPTIMIZATION FOR PUSH SALES RECOMMENDATIONS\")\n",
    "    print(f\"StockPoint: {stock_point_name}, StockPointID: {stock_point_id},\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # STEP 1: Load or Generate Data\n",
    "    print(\"\\n1. Loading Data...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    df_sku_rec, df_customer_dim, df_stockpoint  = data_filter(df_customer_sku_recommendation, \n",
    "                                                                df_master_customer_dim, \n",
    "                                                                df_stockpoint_dim, \n",
    "                                                                stockpoint_id = stock_point_id,  \n",
    "                                                                sku_recency = sku_recency, \n",
    "                                                                customer_recency = customer_recency, \n",
    "                                                                number_recommendation = number_recommendation,\n",
    "                                                                estimate_qty_scale_factor = estimate_qty_scale_factor, \n",
    "                                                                max_estimated_qty = max_estimated_qty,\n",
    "                                                                exclude_recency_customer = exclude_recency_customer)\n",
    "\n",
    "    if len(df_customer_dim) < min_ncust_per_cluster:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df_sku_rec)} SKU recommendations\")\n",
    "    print(f\"✓ Loaded {len(df_customer_dim)} customer records\")\n",
    "    print(f\"✓ Loaded {len(df_stockpoint)} stock points\")\n",
    "\n",
    "    push_recommendation, df_clustering, df_routes, stock_point_coords = cluster_trip_route(df_sku_rec, \n",
    "                                                                                            df_customer_dim, \n",
    "                                                                                            df_stockpoint,\n",
    "                                                                                            stock_point_id,\n",
    "                                                                                            max_customers_per_route, \n",
    "                                                                                            max_volume_per_route,\n",
    "                                                                                            max_distance_km,\n",
    "                                                                                            clustering_method,\n",
    "                                                                                            skip_route_optimization)\n",
    "\n",
    "    ### Cluster Evaluation\n",
    "    try:\n",
    "        evaluate_unsupervised_clustering(df_clustering)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ### Cluster Summary \n",
    "    cluster_summary, df_high_value_cluster_summary, sel_cluster_tuple, sel_total_customer_count = cluster_summary_and_selection(\n",
    "                                                                                                        push_recommendation,\n",
    "                                                                                                        sel_trip_cluster,\n",
    "                                                                                                        min_ncust_per_cluster = min_ncust_per_cluster\n",
    "                                                                                                        )\n",
    "\n",
    "    ## Trip\n",
    "    selected_push_recommendation_trip, all_push_recommendation = prep_selected_trip(push_recommendation, \n",
    "                                                  cluster_summary, \n",
    "                                                  df_master_customer_dim,  \n",
    "                                                  df_stockpoint,\n",
    "                                                  sel_cluster_tuple)\n",
    "    \n",
    " \n",
    "    ### Trip Maps\n",
    "    if skip_route_optimization:\n",
    "        try:\n",
    "            df_selected_trip_summary =  selected_push_recommendation_trip.groupby(['StockPointID','TripID', \n",
    "                                                                                   'CustomerID', 'Latitude','Longitude',\n",
    "                                                                                   'LGA', 'LCDA','CustomerScore']).agg( \n",
    "                        TotalQuantity = ('EstimatedQuantity','sum')\n",
    "                        ,TotalSKU = ('SKUID','nunique')\n",
    "                    ).reset_index()\n",
    "            create_route(df_selected_trip_summary, df_stockpoint)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            save_map_all_clusters(df_routes, df_stockpoint, stock_point_id)\n",
    "        except Exception as e:\n",
    "            print(f'Unable to save the generated map image: {e}')\n",
    "            \n",
    "    else:\n",
    "        try:\n",
    "            run_route_optimizer(df_clustering, sel_cluster_tuple, df_stockpoint, \n",
    "                            stock_point_name,\n",
    "                            sel_total_customer_count, \n",
    "                            capacity_size = 20)\n",
    "        except Exception as e:\n",
    "            print(f'Unable to generate route mapping using orc: {e}')       \n",
    "    \n",
    "\n",
    "    ### Export Data\n",
    "    try:\n",
    "        export_data(\n",
    "                selected_trip = selected_push_recommendation_trip,\n",
    "                all_push_recommendation = all_push_recommendation,\n",
    "                cluster_summary = cluster_summary,\n",
    "                stock_point_name = stock_point_name,\n",
    "                CURRENT_DATE = CURRENT_DATE\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f'Unable to generate route mapping using orc: {e}')\n",
    "\n",
    "    dict_ = {\n",
    "        'stock_point_name': stock_point_name,\n",
    "        'selected_trip': selected_push_recommendation_trip,\n",
    "        'all_push_recommendation': all_push_recommendation,\n",
    "        'cluster_summary': cluster_summary\n",
    "    }\n",
    "\n",
    "    return dict_\n",
    "    #push_recommendation, df_clustering, df_routes, trip_summary, stock_point_coords, df_stockpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44619d0",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cc0a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "df_customer_sku_recommendation_raw = pd.read_feather('./input/customer_sku_recommendation.feather').rename(columns={'FCID':'Stock_Point_ID','CustomerId':'CustomerID'})\n",
    "df_customer_dim_with_affinity_score_raw = pd.read_feather('./input/customer_dim_with_affinity_score.feather').rename(columns={'FCID':'Stock_Point_ID'})\n",
    "df_stockpoint_dim_raw = pd.read_feather('./input/stockpoint_dim.feather')\n",
    "df_kyc_customer = pd.read_feather('./input/kyc_customers.feather')\n",
    "df_customer_score = pd.read_feather('./input/df_customer_score.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1617177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df_customer_sku_recommendation, df_master_customer_dim, df_stockpoint_dim = preprocessing(df_customer_sku_recommendation_raw, \n",
    "                                                                                                        df_customer_dim_with_affinity_score_raw, \n",
    "                                                                                                        df_stockpoint_dim_raw,\n",
    "                                                                                                        df_customer_score,\n",
    "                                                                                                        df_kyc_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe979f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# causeway_customer_dim.query(\"days_since_last_order > 60\")[['KYC_Capture_Status']].value_counts()\n",
    "# causeway_customer_dim.query(\"KYC_Capture_Status == 'No'\")[['days_since_last_order']].hist()#.value_counts()#.reset_index().sort_values('days_since_last_order')\n",
    "# causeway_customer_dim.KYC_Capture_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [col for col in df_stockpoint_dim.Stock_point_Name if 'C' in col] \n",
    "# test_sp = 'OmniHub Apapa Lagos - CAUSEWAY'\n",
    "# # # test_spid = 1647402\n",
    "# # test_sp = 'OmniHub Alimosho Lagos - Barka Agro Mix'\n",
    "# # test_spid = 1647345\n",
    "\n",
    "# df_stockpoint_dim[df_stockpoint_dim['Stock_point_Name'] == test_sp]\n",
    "# # # df_customer_sku_recommendation.query(f'Stock_Point_ID == {test_spid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e3101",
   "metadata": {},
   "source": [
    "### Iterative Run - All SPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_STOCKPOINTS_RESULT = {}\n",
    "for index, row in df_stockpoint_dim.head(10).iterrows():\n",
    "    # if index == 12:\n",
    "    # if index == 5:\n",
    "    stock_point_id =  row['Stock_Point_ID']\n",
    "    stock_point_name = row['Stock_point_Name']\n",
    "    print(f'{index}/{len(df_stockpoint_dim)} \\nStock Point ID: {stock_point_id} || Stock Point Name: {stock_point_name}')  # Access by column name\n",
    "\n",
    "    res_dict = run_push_recommendation(df_customer_sku_recommendation, \n",
    "                                df_master_customer_dim, \n",
    "                                df_stockpoint_dim, \n",
    "                                stock_point_id,\n",
    "                                stock_point_name,\n",
    "                                sku_recency = 7, \n",
    "                                customer_recency = 60, number_recommendation = 10, \n",
    "                                estimate_qty_scale_factor = 1, max_estimated_qty = 5, \n",
    "                                exclude_recency_customer = 5,\n",
    "                                max_customers_per_route=20,\n",
    "                                max_volume_per_route=300,\n",
    "                                max_distance_km = 5,\n",
    "                                sel_trip_cluster = 4,\n",
    "                                min_ncust_per_cluster = 5,\n",
    "                                clustering_method = 'divisive',\n",
    "                                skip_route_optimization = True)\n",
    "    \n",
    "    ALL_STOCKPOINTS_RESULT[stock_point_name] = res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d067f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_dict.keys())\n",
    "# [col  for col in res_dict['all_push_recommendation'].columns if 'KYC_Capture_Status' in col]\n",
    "# res_dict['cluster_summary'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b199e2",
   "metadata": {},
   "source": [
    "# Data Export to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL_STOCKPOINTS_RESULT.keys()\n",
    "# # ALL_STOCKPOINTS_RESULT['OmniHub Obio Akpor Rivers - Rivoc']\n",
    "# ALL_STOCKPOINTS_RESULT['OmniHub Ado Odo/Ota Ogun - Prince Tunadek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ALL_RECOMMENDATION = pd.DataFrame() \n",
    "for key in ALL_STOCKPOINTS_RESULT.keys():\n",
    "    dict_f = ALL_STOCKPOINTS_RESULT[key]\n",
    "    if dict_f != {}:\n",
    "        df_ = dict_f['all_push_recommendation']\n",
    "    \n",
    "        if len(df_) > 0:\n",
    "            df_['ClusterLGAs'] = df_['ClusterLGAs'].apply(str)\n",
    "            df_['ClusterLCDAs'] = df_['ClusterLCDAs'].apply(str)\n",
    "            DF_ALL_RECOMMENDATION = pd.concat([DF_ALL_RECOMMENDATION, df_])\n",
    "            DF_ALL_RECOMMENDATION['ModifiedDate'] = CURRENT_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_summary = ['StockPointID', 'StockPointName', 'TripID', 'ClusterLGAs', 'ClusterLCDAs', 'TotalCustonerCount', 'TripTotalQuantity','TripAvgCustomerScore', 'ModifiedDate']  \n",
    "DF_CLUSTER_SUMMARY = DF_ALL_RECOMMENDATION[cols_summary].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# DF_ALL_RECOMMENDATION.sample(1)\n",
    "DF_CLUSTER_SUMMARY.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ba250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lengths = DF_ALL_RECOMMENDATION.astype(str).applymap(len).max().reset_index(name = 'max_length')\n",
    "# print(max_lengths)\n",
    "# print(DF_ALL_RECOMMENDATION.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15686f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_dataframe(df, table_name, conn, match_cols, update_cols, batch_size = 2000, fast_executemany = True):\n",
    "    # Input validation\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty.\")\n",
    "    if not match_cols or not update_cols:\n",
    "        raise ValueError(\"match_cols and update_cols cannot be empty.\")\n",
    "    if not all(col in df.columns for col in match_cols + update_cols):\n",
    "        raise ValueError(\"Some match_cols or update_cols are not in the DataFrame.\")\n",
    "    if not table_name.strip() or any(c in table_name for c in \".;[]\"):\n",
    "        raise ValueError(\"Invalid table_name.\")\n",
    "\n",
    "    cursor = None\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.fast_executemany = fast_executemany \n",
    "\n",
    "        staging_table = f\"#{table_name}_staging\"\n",
    "        cols = df.columns.tolist()\n",
    "        col_list = ', '.join(f\"[{col}]\" for col in cols)\n",
    "        placeholders = ', '.join(['?'] * len(cols))\n",
    "\n",
    "        # Step 1: Create staging table from real schema\n",
    "        create_staging_sql = f\"\"\"\n",
    "        SELECT TOP 0 {col_list}\n",
    "        INTO {staging_table}\n",
    "        FROM {table_name}\n",
    "        WHERE 1 = 0;\n",
    "        \"\"\"\n",
    "        cursor.execute(create_staging_sql)\n",
    "\n",
    "        # Step 2: Bulk insert into staging table using fast_executemany\n",
    "        insert_sql = f\"INSERT INTO {staging_table} ({col_list}) VALUES ({placeholders})\"\n",
    "        cursor.executemany(insert_sql, df[cols].values.tolist())\n",
    "        conn.commit()\n",
    "\n",
    "        # insert_sql = f\"INSERT INTO {staging_table} ({col_list}) VALUES ({placeholders})\"\n",
    "        # data = df[cols].values.tolist()\n",
    "        # for i in range(0, len(data), batch_size):\n",
    "        #     cursor.executemany(insert_sql, data[i:i+batch_size])\n",
    "        # conn.commit()\n",
    "\n",
    "        # Step 3: MERGE for upsert\n",
    "        on_clause = ' AND '.join([f\"TARGET.[{col}] = SOURCE.[{col}]\" for col in match_cols])\n",
    "        update_clause = ', '.join([f\"TARGET.[{col}] = SOURCE.[{col}]\" for col in update_cols])\n",
    "        insert_cols = ', '.join([f\"[{col}]\" for col in cols])\n",
    "        insert_values = ', '.join([f\"SOURCE.[{col}]\" for col in cols])\n",
    "\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE {table_name} AS TARGET\n",
    "        USING {staging_table} AS SOURCE\n",
    "        ON {on_clause}\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET {update_clause}\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ({insert_cols})\n",
    "            VALUES ({insert_values});\n",
    "        \"\"\"\n",
    "        cursor.execute(merge_sql)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        logger.error(f\"Upsert failed for table {table_name}: {e}\")\n",
    "        raise Exception(f\"Upsert failed for table {table_name}: {e}\") from e\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert = DF_ALL_RECOMMENDATION.drop(columns=['ClusterLGAs',\t'ClusterLCDAs']).reset_index(drop=True)\n",
    "# Replace 'YourProblematicColumn' with the actual column name you found\n",
    "df_insert['SKUDaysSinceLastBuy'] = df_insert['SKUDaysSinceLastBuy'].astype(float, errors='ignore').astype(int, errors='ignore')\n",
    "df_insert['CustomerDaysSinceLastBuy'] = df_insert['CustomerDaysSinceLastBuy'].astype(float, errors='ignore').astype(int, errors='ignore')\n",
    "\n",
    "match_cols = ['StockPointID', 'CustomerID', 'SKUID', 'ModifiedDate']\n",
    "update_cols = list(set(df_insert.columns) - set(match_cols))\n",
    "\n",
    "conn = get_connection()\n",
    "upsert_dataframe(\n",
    "    df=df_insert,\n",
    "    table_name='dailyPredictedPull',\n",
    "    conn=conn,\n",
    "    match_cols=match_cols,\n",
    "    update_cols=update_cols   \n",
    ") \n",
    " \n",
    "print(conn.closed)\n",
    "conn.close()\n",
    "print(conn.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DF_CLUSTER_SUMMARY.copy() \n",
    "\n",
    "df['ClusterLGAs'] = df['ClusterLGAs'].astype(str).str.slice(0, 500)\n",
    "df['ClusterLCDAs'] = df['ClusterLCDAs'].astype(str).str.slice(0, 500)\n",
    "\n",
    "\n",
    "match_cols = ['StockPointID', 'TripID', 'ModifiedDate']\n",
    "update_cols = list(set(df.columns) - set(match_cols))\n",
    "\n",
    "conn = get_connection()\n",
    "upsert_dataframe(\n",
    "    df=df,\n",
    "    table_name='dailyPredictedPullClusterSummary',\n",
    "    conn=conn,\n",
    "    match_cols=match_cols,\n",
    "    update_cols=update_cols ,\n",
    "    fast_executemany = False  \n",
    ")\n",
    "\n",
    "print(conn.closed)\n",
    "conn.close()\n",
    "print(conn.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa188a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# res_dict['all_push_recommendation'].sample(1)\n",
    "# # res_dict['cluster_summary'].sort_values('ncustomer', ascending = False)\n",
    "# from collections import Counter\n",
    "# Counter(res_dict['all_push_recommendation'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210a690",
   "metadata": {},
   "source": [
    "# Case Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cced9601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Quantity before filter: 29,872\n",
      "Total Quantity: 19,084\n",
      "Total Number of Customers before filter: 1,063\n",
      "Total Number of Customers: 679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(679, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Filter - Testing \n",
    "# stockpoint_id = 1647394\n",
    "stock_point_id = 1647113\n",
    "\n",
    "# causeway, causeway_customer_dim, causeway_stockpoint, = data_filter(df_customer_sku_recommendation, \n",
    "df_sku_rec, df_customer_dim, df_stockpoint = data_filter(df_customer_sku_recommendation,          \n",
    "                                                                    df_master_customer_dim, \n",
    "                                                                    df_stockpoint_dim, \n",
    "                                                                    stockpoint_id = stock_point_id,   \n",
    "                                                                    sku_recency = 7, customer_recency = 60, number_recommendation = 10,\n",
    "                                                                    estimate_qty_scale_factor = 1, max_estimated_qty = 5, \n",
    "                                                                    exclude_recency_customer = 4)\n",
    "\n",
    "# Total Number of Customers: 905 || 901\n",
    "\n",
    "df_all_cluster = df_customer_dim[['CustomerID', 'Latitude','Longitude']].drop_duplicates()\n",
    "df_all_cluster.shape\n",
    "# df_all_cluster = res_dict['all_push_recommendation'][['CustomerID','TripID', 'Latitude','Longitude']].drop_duplicates().rename(columns={'TripID':'cluster'})\n",
    "\n",
    "\n",
    "# vis_and_save(df_routes = df_all_cluster,\n",
    "#                  df_stockpoint = None,   \n",
    "#                  filename=None,\n",
    "#                  cluster_col = 'cluster')\n",
    "\n",
    "# 459\n",
    "# 364 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb813416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_cluster.Latitude.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a397ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['CustomerID', 'SKUID', 'Medium','CustomerPurchaseRecency']\n",
    "# causeway[cols].query('(CustomerID == 5271729) or (CustomerID ==  5266873)')\n",
    "# causeway.groupby(['CustomerPurchaseRecency'])['CustomerID'].nunique().reset_index().sort_values('CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a0bf43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Route optimizer initialized\n",
      "\n",
      "3. Generating Optimized Routes...\n",
      "----------------------------------------\n",
      "\n",
      "4. Route Analysis & Results...\n",
      "----------------------------------------\n",
      "Silhouette Score: 0.32\n",
      "Davies-Bouldin Index: 0.84\n",
      "Calinski-Harabasz Score: 10067.68\n"
     ]
    }
   ],
   "source": [
    "## Create Cluster\n",
    "sku_recency = 7\n",
    "customer_recency = 60 \n",
    "number_recommendation = 5\n",
    "estimate_qty_scale_factor = 1\n",
    "max_estimated_qty = 5\n",
    "exclude_recency_customer = 4\n",
    "max_customers_per_route=20\n",
    "max_volume_per_route=300\n",
    "max_distance_km = 40\n",
    "sel_trip_cluster = 5\n",
    "min_ncust_per_cluster = 5\n",
    "clustering_method = 'divisive'\n",
    "skip_route_optimization = True\n",
    "\n",
    "push_recommendation, df_clustering, df_routes, stock_point_coords = cluster_trip_route(df_sku_rec = df_sku_rec, \n",
    "                                                                        df_customer_dim = df_customer_dim, \n",
    "                                                                        df_stockpoint = df_stockpoint,\n",
    "                                                                        stock_point_id = stock_point_id,\n",
    "                                                                        max_customers_per_route = max_customers_per_route, \n",
    "                                                                        max_volume_per_route = max_volume_per_route,\n",
    "                                                                        max_distance_km = max_distance_km, \n",
    "                                                                        clustering_method='divisive',\n",
    "                                                                        skip_route_optimization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "501424d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_map_all_clusters(df_routes, df_stockpoint, stock_point_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52166af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca\n",
    "\n",
    "# Define your HTML content\n",
    "html_content = \"\"\"\n",
    "<div style='font-family: Arial, sans-serif; font-size: 12px;'>\n",
    "    <div style='font-weight: bold; color: #1f77b4; margin-bottom: 5px;'>Cluster 1</div>\n",
    "    <div style='margin: 2px 0;'><strong>CustomerID:</strong> 5301162</div>\n",
    "    <div style='margin: 2px 0;'><strong>LGA:</strong> AMAC 1</div>\n",
    "    <div style='margin: 2px 0;'><strong>LCDA:</strong> Idu</div>\n",
    "    <div style='margin: 2px 0;'><strong>Coordinates:</strong> 9.0800, 7.3838</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Create an IFrame\n",
    "iframe = branca.element.IFrame(html=html_content, width=300, height=200)\n",
    "\n",
    "# Create a popup with the IFrame\n",
    "popup = folium.Popup(iframe, parse_html=True)\n",
    "\n",
    "# Create a map and add the popup\n",
    "m = folium.Map(location=[9.0800, 7.3838], zoom_start=12)\n",
    "folium.Marker([9.0800, 7.3838], popup=popup).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('map.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73d94d",
   "metadata": {},
   "source": [
    "### Test New Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee242d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering.divisive_clustering import DivisiveGeographicClustering, OptimizedDivisiveGeographicClustering\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. DIVISIVE HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "divisive_clusterer = OptimizedDivisiveGeographicClustering( \n",
    "    # Rivers: Divisive clusters created: 48 || Silhouette Score: 0.54 || Constraint violations: Size=6, Distance=3\n",
    "    max_customers_per_cluster=30,  # REQUIRED\n",
    "    max_distance_km=5            # REQUIRED\n",
    "    ,use_vectorized_distances=True, balance_clusters=False\n",
    ")\n",
    "\n",
    "divisive_result = divisive_clusterer.divisive_clustering(df_all_cluster.copy())\n",
    "print(f\"\\nDivisive clusters created: {divisive_result['cluster'].nunique()}\")\n",
    "# print(f\"Cluster sizes: {divisive_result['cluster'].value_counts().sort_index().head()}\")\n",
    "_ = evaluate_unsupervised_clustering(divisive_result)\n",
    "\n",
    "# # Get detailed statistics\n",
    "stats = divisive_clusterer.get_cluster_stats(divisive_result)\n",
    "print(f\"Total clusters: {stats['summary']['total_clusters']}\")\n",
    "print(f\"Constraint violations: Size={stats['summary']['size_violations']}, Distance={stats['summary']['distance_violations']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc977318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering.agglomerative_clustering import AgglomerativeGeographicClustering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. AGGLOMERATIVE CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    " \n",
    "agg_clusterer = AgglomerativeGeographicClustering(\n",
    "    max_customers_per_cluster=30, # Aim for clusters of max 50 customers\n",
    "    max_distance_km=5.0,        # Max diameter of 5 km\n",
    "    linkage_method='ward',       # Common choice for compact clusters\n",
    "    sub_cluster_if_oversized=True\n",
    ")\n",
    "clustered_agg_df = agg_clusterer.agglomerative_clustering(df_all_cluster.copy())\n",
    "\n",
    "# print(\"\\nAgglomerative Clustering Stats:\")\n",
    "# for k, v in agg_stats['summary'].items():\n",
    "#     print(f\"  {k}: {v}\")\n",
    "# print(\"\\nSample Agglomerative Cluster Details:\")\n",
    "# for cluster_id, details in list(agg_stats.items())[:5]: # Show first 5 clusters\n",
    "#     if cluster_id != 'summary':\n",
    "#         print(f\"  Cluster {cluster_id}: Size={details['size']}, Diameter={details['diameter_km']:.2f} km, Size OK={details['meets_size_constraint']}, Distance OK={details['meets_distance_constraint']}\")\n",
    "\n",
    "\n",
    "_ = evaluate_unsupervised_clustering(clustered_agg_df)\n",
    "\n",
    "\n",
    "stats = agg_clusterer.get_cluster_stats(clustered_agg_df, agg_clusterer.max_customers_per_cluster, agg_clusterer.max_distance_km) \n",
    "print(f\"Total clusters: {stats['summary']['total_clusters']}\")\n",
    "print(f\"Constraint violations: Size={stats['summary']['size_violations']}, Distance={stats['summary']['distance_violations']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_and_save(df_routes = clustered_agg_df,\n",
    "                 df_stockpoint = None,   \n",
    "                 filename='agglomerative-clustering-test.html',\n",
    "                 cluster_col = 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a73b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_and_save(df_routes = divisive_result,\n",
    "                 df_stockpoint = None,   \n",
    "                 filename='divisive-clustering-test.html',\n",
    "                 cluster_col = 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08980d8e",
   "metadata": {},
   "source": [
    "### New Clustering Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Best\n",
    "class OptimizedDivisiveGeographicClustering_b:\n",
    "    def __init__(self, max_customers_per_cluster=20, max_distance_km=50, \n",
    "                 use_vectorized_distances=True, balance_clusters=False):\n",
    "        self.max_customers_per_cluster = max_customers_per_cluster\n",
    "        self.max_distance_km = max_distance_km\n",
    "        self.earth_radius_km = 6371.0\n",
    "        self.use_vectorized_distances = use_vectorized_distances\n",
    "        self.balance_clusters = balance_clusters\n",
    "        \n",
    "    def haversine_vectorized(self, coords1, coords2=None):\n",
    "        \"\"\"\n",
    "        Highly optimized vectorized haversine distance calculation.\n",
    "        If coords2 is None, calculates pairwise distances within coords1.\n",
    "        \"\"\"\n",
    "        if coords2 is None:\n",
    "            # Pairwise distances within coords1\n",
    "            coords1_rad = np.radians(coords1)\n",
    "            n = len(coords1)\n",
    "            \n",
    "            # Create meshgrids for vectorized calculation\n",
    "            lat1 = coords1_rad[:, 0]\n",
    "            lon1 = coords1_rad[:, 1]\n",
    "            \n",
    "            lat1_mesh, lat2_mesh = np.meshgrid(lat1, lat1)\n",
    "            lon1_mesh, lon2_mesh = np.meshgrid(lon1, lon1)\n",
    "            \n",
    "            dlat = lat2_mesh - lat1_mesh\n",
    "            dlon = lon2_mesh - lon1_mesh\n",
    "            \n",
    "            a = (np.sin(dlat / 2) ** 2 + \n",
    "                 np.cos(lat1_mesh) * np.cos(lat2_mesh) * np.sin(dlon / 2) ** 2)\n",
    "            c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "            \n",
    "            return self.earth_radius_km * c\n",
    "        else:\n",
    "            # Distance from each point in coords1 to each point in coords2\n",
    "            coords1_rad = np.radians(coords1)\n",
    "            coords2_rad = np.radians(coords2)\n",
    "            \n",
    "            lat1 = coords1_rad[:, 0][:, np.newaxis]\n",
    "            lon1 = coords1_rad[:, 1][:, np.newaxis]\n",
    "            lat2 = coords2_rad[:, 0][np.newaxis, :]\n",
    "            lon2 = coords2_rad[:, 1][np.newaxis, :]\n",
    "            \n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            \n",
    "            a = (np.sin(dlat / 2) ** 2 + \n",
    "                 np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "            c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "            \n",
    "            return self.earth_radius_km * c\n",
    "    \n",
    "    def haversine_pdist(self, coords):\n",
    "        \"\"\"\n",
    "        Optimized haversine distance calculation using scipy's pdist.\n",
    "        \"\"\"\n",
    "        def haversine_metric(u, v):\n",
    "            lat1, lon1 = np.radians(u)\n",
    "            lat2, lon2 = np.radians(v)\n",
    "            \n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            \n",
    "            a = (np.sin(dlat / 2) ** 2 + \n",
    "                 np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "            c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "            \n",
    "            return self.earth_radius_km * c\n",
    "        \n",
    "        return pdist(coords, metric=haversine_metric)\n",
    "    \n",
    "    def calculate_cluster_diameter_fast(self, coords):\n",
    "        \"\"\"\n",
    "        Fast cluster diameter calculation with multiple optimization strategies.\n",
    "        \"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        if n_points <= 1:\n",
    "            return 0\n",
    "        \n",
    "        if n_points == 2:\n",
    "            return self.haversine_single_pair(coords[0], coords[1])\n",
    "        \n",
    "        # Use different strategies based on cluster size\n",
    "        if n_points <= 10:\n",
    "            # Small clusters: exact calculation\n",
    "            distances = self.haversine_pdist(coords)\n",
    "            return np.max(distances)\n",
    "        elif n_points <= 50:\n",
    "            # Medium clusters: vectorized calculation\n",
    "            if self.use_vectorized_distances:\n",
    "                distance_matrix = self.haversine_vectorized(coords)\n",
    "                return np.max(distance_matrix)\n",
    "            else:\n",
    "                distances = self.haversine_pdist(coords)\n",
    "                return np.max(distances)\n",
    "        else:\n",
    "            # Large clusters: smart sampling\n",
    "            return self._smart_diameter_estimation(coords)\n",
    "    \n",
    "    def _smart_diameter_estimation(self, coords):\n",
    "        \"\"\"\n",
    "        Improved diameter estimation using multiple sampling strategies.\n",
    "        \"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        # Strategy 1: Convex hull approximation\n",
    "        hull_diameter = self._convex_hull_diameter(coords)\n",
    "        \n",
    "        # Strategy 2: Grid-based sampling for large clusters\n",
    "        if n_points > 200:\n",
    "            grid_diameter = self._grid_based_diameter(coords)\n",
    "            return max(hull_diameter, grid_diameter)\n",
    "        \n",
    "        return hull_diameter\n",
    "    \n",
    "    def _convex_hull_diameter(self, coords):\n",
    "        \"\"\"Enhanced convex hull approximation.\"\"\"\n",
    "        lats, lons = coords[:, 0], coords[:, 1]\n",
    "        \n",
    "        # Get extreme points\n",
    "        extreme_indices = [\n",
    "            np.argmax(lats), np.argmin(lats),\n",
    "            np.argmax(lons), np.argmin(lons)\n",
    "        ]\n",
    "        \n",
    "        # Add points from different quadrants\n",
    "        lat_center, lon_center = np.mean(lats), np.mean(lons)\n",
    "        \n",
    "        quadrants = [\n",
    "            (lats >= lat_center) & (lons >= lon_center),  # NE\n",
    "            (lats >= lat_center) & (lons < lon_center),   # NW\n",
    "            (lats < lat_center) & (lons >= lon_center),   # SE\n",
    "            (lats < lat_center) & (lons < lon_center)     # SW\n",
    "        ]\n",
    "        \n",
    "        for quadrant in quadrants:\n",
    "            if np.any(quadrant):\n",
    "                quad_indices = np.where(quadrant)[0]\n",
    "                # Add furthest point from center in each quadrant\n",
    "                distances_from_center = np.sqrt(\n",
    "                    (lats[quad_indices] - lat_center)**2 + \n",
    "                    (lons[quad_indices] - lon_center)**2\n",
    "                )\n",
    "                furthest_idx = quad_indices[np.argmax(distances_from_center)]\n",
    "                extreme_indices.append(furthest_idx)\n",
    "        \n",
    "        # Add some random points\n",
    "        n_random = min(12, len(coords) - len(set(extreme_indices)))\n",
    "        if n_random > 0:\n",
    "            available_indices = list(set(range(len(coords))) - set(extreme_indices))\n",
    "            if available_indices:\n",
    "                random_indices = np.random.choice(available_indices, \n",
    "                                                min(n_random, len(available_indices)), \n",
    "                                                replace=False)\n",
    "                extreme_indices.extend(random_indices)\n",
    "        \n",
    "        # Get unique sample\n",
    "        sample_indices = list(set(extreme_indices))\n",
    "        sample_coords = coords[sample_indices]\n",
    "        \n",
    "        if len(sample_coords) <= 1:\n",
    "            return 0\n",
    "        \n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        return np.max(distances)\n",
    "    \n",
    "    def _grid_based_diameter(self, coords):\n",
    "        \"\"\"Grid-based sampling for very large clusters.\"\"\"\n",
    "        # Create a grid and sample points from each grid cell\n",
    "        lats, lons = coords[:, 0], coords[:, 1]\n",
    "        \n",
    "        # Create 6x6 grid\n",
    "        lat_bins = np.linspace(lats.min(), lats.max(), 7)\n",
    "        lon_bins = np.linspace(lons.min(), lons.max(), 7)\n",
    "        \n",
    "        sample_indices = []\n",
    "        for i in range(len(lat_bins)-1):\n",
    "            for j in range(len(lon_bins)-1):\n",
    "                mask = ((lats >= lat_bins[i]) & (lats < lat_bins[i+1]) & \n",
    "                       (lons >= lon_bins[j]) & (lons < lon_bins[j+1]))\n",
    "                cell_indices = np.where(mask)[0]\n",
    "                if len(cell_indices) > 0:\n",
    "                    # Sample up to 2 points from each cell\n",
    "                    n_sample = min(2, len(cell_indices))\n",
    "                    sampled = np.random.choice(cell_indices, n_sample, replace=False)\n",
    "                    sample_indices.extend(sampled)\n",
    "        \n",
    "        if len(sample_indices) <= 1:\n",
    "            return 0\n",
    "        \n",
    "        sample_coords = coords[sample_indices]\n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        return np.max(distances)\n",
    "    \n",
    "    def haversine_single_pair(self, coord1, coord2):\n",
    "        \"\"\"Calculate haversine distance between two points.\"\"\"\n",
    "        lat1, lon1 = np.radians(coord1)\n",
    "        lat2, lon2 = np.radians(coord2)\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = (np.sin(dlat / 2) ** 2 + \n",
    "             np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "        c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "        \n",
    "        return self.earth_radius_km * c\n",
    "    \n",
    "    def should_split_cluster(self, cluster_indices, coords_array):\n",
    "        \"\"\"Enhanced cluster splitting logic with load balancing.\"\"\"\n",
    "        cluster_size = len(cluster_indices)\n",
    "        \n",
    "        if cluster_size <= 2:\n",
    "            return False\n",
    "        \n",
    "        # Hard size constraint\n",
    "        if cluster_size > self.max_customers_per_cluster * 1.5:\n",
    "            return True\n",
    "        \n",
    "        # Soft size constraint with diameter check\n",
    "        if cluster_size > self.max_customers_per_cluster:\n",
    "            cluster_coords = coords_array[cluster_indices]\n",
    "            diameter = self.calculate_cluster_diameter_fast(cluster_coords)\n",
    "            return diameter > self.max_distance_km * 0.8  # More lenient for size\n",
    "        \n",
    "        # Diameter constraint\n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        diameter = self.calculate_cluster_diameter_fast(cluster_coords)\n",
    "        \n",
    "        return diameter > self.max_distance_km\n",
    "    \n",
    "    def geographic_split(self, cluster_indices, coords_array):\n",
    "        \"\"\"\n",
    "        Improved geographic splitting with better load balancing.\n",
    "        \"\"\"\n",
    "        if len(cluster_indices) <= 2:\n",
    "            return [cluster_indices]\n",
    "        \n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        n_points = len(cluster_coords)\n",
    "        \n",
    "        # For small clusters, use exact method\n",
    "        if n_points <= 50:\n",
    "            return self._exact_geographic_split(cluster_indices, cluster_coords)\n",
    "        \n",
    "        # For medium clusters, use K-means with geographic initialization\n",
    "        if n_points <= 200:\n",
    "            return self._kmeans_geographic_split(cluster_indices, cluster_coords)\n",
    "        \n",
    "        # For large clusters, use hierarchical approach\n",
    "        return self._hierarchical_geographic_split(cluster_indices, cluster_coords)\n",
    "    \n",
    "    def _exact_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Exact splitting for small clusters.\"\"\"\n",
    "        n_points = len(cluster_coords)\n",
    "        \n",
    "        # Find the two points that are farthest apart\n",
    "        distances = self.haversine_pdist(cluster_coords)\n",
    "        distance_matrix = squareform(distances)\n",
    "        max_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "        center1_idx, center2_idx = max_idx[0], max_idx[1]\n",
    "        \n",
    "        center1 = cluster_coords[center1_idx]\n",
    "        center2 = cluster_coords[center2_idx]\n",
    "        \n",
    "        # Assign points to closest center\n",
    "        distances_to_center1 = self.haversine_vectorized(cluster_coords, center1.reshape(1, -1))[:, 0]\n",
    "        distances_to_center2 = self.haversine_vectorized(cluster_coords, center2.reshape(1, -1))[:, 0]\n",
    "        \n",
    "        labels = (distances_to_center1 <= distances_to_center2).astype(int)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "    \n",
    "    def _kmeans_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"K-means splitting with geographic initialization.\"\"\"\n",
    "        n_points = len(cluster_coords)\n",
    "        \n",
    "        # Initialize with farthest pair\n",
    "        center1_idx, center2_idx = self._find_approximate_farthest_pair(cluster_coords)\n",
    "        initial_centers = cluster_coords[[center1_idx, center2_idx]]\n",
    "        \n",
    "        # Apply K-means\n",
    "        kmeans = KMeans(n_clusters=2, init=initial_centers, n_init=1, random_state=42)\n",
    "        labels = kmeans.fit_predict(cluster_coords)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "    \n",
    "    def _hierarchical_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Hierarchical splitting for large clusters.\"\"\"\n",
    "        # Use linkage-based clustering for very large clusters\n",
    "        n_sample = min(100, len(cluster_coords))\n",
    "        sample_indices = np.random.choice(len(cluster_coords), n_sample, replace=False)\n",
    "        sample_coords = cluster_coords[sample_indices]\n",
    "        \n",
    "        # Compute linkage on sample\n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        linkage_matrix = linkage(distances, method='ward')\n",
    "        sample_labels = fcluster(linkage_matrix, 2, criterion='maxclust') - 1\n",
    "        \n",
    "        # Assign all points based on closest sample point\n",
    "        center1_coords = sample_coords[sample_labels == 0]\n",
    "        center2_coords = sample_coords[sample_labels == 1]\n",
    "        \n",
    "        if len(center1_coords) == 0 or len(center2_coords) == 0:\n",
    "            # Fallback to farthest pair method\n",
    "            return self._exact_geographic_split(cluster_indices, cluster_coords)\n",
    "        \n",
    "        center1 = np.mean(center1_coords, axis=0)\n",
    "        center2 = np.mean(center2_coords, axis=0)\n",
    "        \n",
    "        distances_to_center1 = self.haversine_vectorized(cluster_coords, center1.reshape(1, -1))[:, 0]\n",
    "        distances_to_center2 = self.haversine_vectorized(cluster_coords, center2.reshape(1, -1))[:, 0]\n",
    "        \n",
    "        labels = (distances_to_center1 <= distances_to_center2).astype(int)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "    \n",
    "    def _balance_split(self, cluster_indices, labels):\n",
    "        \"\"\"Balance the split to avoid very uneven clusters.\"\"\"\n",
    "        cluster_0_indices = cluster_indices[labels == 0]\n",
    "        cluster_1_indices = cluster_indices[labels == 1]\n",
    "        \n",
    "        # Ensure no empty clusters\n",
    "        if len(cluster_0_indices) == 0:\n",
    "            cluster_0_indices = np.array([cluster_1_indices[0]])\n",
    "            cluster_1_indices = cluster_1_indices[1:]\n",
    "        elif len(cluster_1_indices) == 0:\n",
    "            cluster_1_indices = np.array([cluster_0_indices[0]])\n",
    "            cluster_0_indices = cluster_0_indices[1:]\n",
    "        \n",
    "        # Optional: Balance cluster sizes if one is much larger\n",
    "        if self.balance_clusters:\n",
    "            size_0, size_1 = len(cluster_0_indices), len(cluster_1_indices)\n",
    "            if size_0 > 3 * size_1 and size_1 > 0:\n",
    "                # Move some points from cluster 0 to cluster 1\n",
    "                n_move = (size_0 - size_1) // 4\n",
    "                move_indices = cluster_0_indices[:n_move]\n",
    "                cluster_0_indices = cluster_0_indices[n_move:]\n",
    "                cluster_1_indices = np.concatenate([cluster_1_indices, move_indices])\n",
    "            elif size_1 > 3 * size_0 and size_0 > 0:\n",
    "                # Move some points from cluster 1 to cluster 0\n",
    "                n_move = (size_1 - size_0) // 4\n",
    "                move_indices = cluster_1_indices[:n_move]\n",
    "                cluster_1_indices = cluster_1_indices[n_move:]\n",
    "                cluster_0_indices = np.concatenate([cluster_0_indices, move_indices])\n",
    "        \n",
    "        return [cluster_0_indices, cluster_1_indices]\n",
    "    \n",
    "    def _find_approximate_farthest_pair(self, coords):\n",
    "        \"\"\"Find approximate farthest pair for large clusters.\"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        if n_points <= 100:\n",
    "            # For moderate sizes, use exact calculation\n",
    "            distances = self.haversine_pdist(coords)\n",
    "            distance_matrix = squareform(distances)\n",
    "            max_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "            return max_idx[0], max_idx[1]\n",
    "        \n",
    "        # For large clusters, use sampling\n",
    "        sample_size = min(50, n_points)\n",
    "        sample_indices = np.random.choice(n_points, sample_size, replace=False)\n",
    "        sample_coords = coords[sample_indices]\n",
    "        \n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        distance_matrix = squareform(distances)\n",
    "        max_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "        \n",
    "        return sample_indices[max_idx[0]], sample_indices[max_idx[1]]\n",
    "    \n",
    "    def divisive_clustering(self, customers_df):\n",
    "        \"\"\"Perform optimized divisive hierarchical clustering.\"\"\"\n",
    "        customers_df = customers_df.copy().reset_index(drop=True)\n",
    "        \n",
    "        # Validate input\n",
    "        if 'Latitude' not in customers_df.columns or 'Longitude' not in customers_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'Latitude' and 'Longitude' columns\")\n",
    "        \n",
    "        coords_array = customers_df[['Latitude', 'Longitude']].values\n",
    "        n_customers = len(customers_df)\n",
    "        \n",
    "        if n_customers == 0:\n",
    "            customers_df['cluster'] = []\n",
    "            return customers_df\n",
    "        \n",
    "        if n_customers == 1:\n",
    "            customers_df['cluster'] = 1\n",
    "            return customers_df\n",
    "        \n",
    "        # Priority queue approach for better clustering\n",
    "        clusters_to_process = [(n_customers, np.arange(n_customers))]  # (size, indices)\n",
    "        final_clusters = []\n",
    "        \n",
    "        iteration_count = 0\n",
    "        max_iterations = n_customers * 2\n",
    "        \n",
    "        while clusters_to_process and iteration_count < max_iterations:\n",
    "            # Process largest cluster first\n",
    "            clusters_to_process.sort(key=lambda x: x[0], reverse=True)\n",
    "            current_size, current_cluster_indices = clusters_to_process.pop(0)\n",
    "            iteration_count += 1\n",
    "            \n",
    "            if self.should_split_cluster(current_cluster_indices, coords_array):\n",
    "                subclusters = self.geographic_split(current_cluster_indices, coords_array)\n",
    "                \n",
    "                for subcluster_indices in subclusters:\n",
    "                    if len(subcluster_indices) > 0:\n",
    "                        clusters_to_process.append((len(subcluster_indices), subcluster_indices))\n",
    "            else:\n",
    "                final_clusters.append(current_cluster_indices)\n",
    "        \n",
    "        # Handle remaining clusters\n",
    "        final_clusters.extend([indices for _, indices in clusters_to_process])\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result_df = customers_df.copy()\n",
    "        result_df['cluster'] = -1\n",
    "        \n",
    "        for cluster_id, cluster_indices in enumerate(final_clusters, 1):\n",
    "            result_df.loc[cluster_indices, 'cluster'] = cluster_id\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def get_cluster_stats(self, clustered_df):\n",
    "        \"\"\"Get comprehensive clustering statistics.\"\"\"\n",
    "        if 'cluster' not in clustered_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'cluster' column\")\n",
    "        \n",
    "        stats = {}\n",
    "        cluster_sizes = []\n",
    "        cluster_diameters = []\n",
    "        \n",
    "        for cluster_id in clustered_df['cluster'].unique():\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "                \n",
    "            cluster_data = clustered_df[clustered_df['cluster'] == cluster_id]\n",
    "            coords = cluster_data[['Latitude', 'Longitude']].values\n",
    "            \n",
    "            diameter = self.calculate_cluster_diameter_fast(coords)\n",
    "            cluster_sizes.append(len(cluster_data))\n",
    "            cluster_diameters.append(diameter)\n",
    "            \n",
    "            stats[cluster_id] = {\n",
    "                'size': len(cluster_data),\n",
    "                'diameter_km': diameter,\n",
    "                'centroid_lat': np.mean(coords[:, 0]),\n",
    "                'centroid_lon': np.mean(coords[:, 1]),\n",
    "                'meets_size_constraint': len(cluster_data) <= self.max_customers_per_cluster,\n",
    "                'meets_distance_constraint': diameter <= self.max_distance_km\n",
    "            }\n",
    "        \n",
    "        # Overall statistics\n",
    "        stats['summary'] = {\n",
    "            'total_clusters': len(stats) - 1,  # Excluding summary\n",
    "            'avg_cluster_size': np.mean(cluster_sizes),\n",
    "            'max_cluster_size': np.max(cluster_sizes),\n",
    "            'min_cluster_size': np.min(cluster_sizes),\n",
    "            'avg_diameter': np.mean(cluster_diameters),\n",
    "            'max_diameter': np.max(cluster_diameters),\n",
    "            'size_violations': sum(1 for size in cluster_sizes if size > self.max_customers_per_cluster),\n",
    "            'distance_violations': sum(1 for diameter in cluster_diameters if diameter > self.max_distance_km)\n",
    "        }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40e0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from numba import jit, prange\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fixed Enhanced Geographic Clustering\n",
    "class OptimizedDivisiveGeographicClustering_be:\n",
    "    def __init__(self, max_customers_per_cluster=20, max_distance_km=50, \n",
    "                 use_vectorized_distances=True, balance_clusters=False):\n",
    "        self.max_customers_per_cluster = max_customers_per_cluster\n",
    "        self.max_distance_km = max_distance_km\n",
    "        self.earth_radius_km = 6371.0\n",
    "        self.use_vectorized_distances = use_vectorized_distances\n",
    "        self.balance_clusters = balance_clusters\n",
    "        self._distance_cache = {}\n",
    "        \n",
    "    @staticmethod\n",
    "    @jit(nopython=True, fastmath=True, parallel=True)\n",
    "    def numba_haversine(coords1, coords2, radius):\n",
    "        \"\"\"JIT-optimized haversine distance calculation\"\"\"\n",
    "        n1 = coords1.shape[0]\n",
    "        n2 = coords2.shape[0] if coords2 is not None else n1\n",
    "        dists = np.empty((n1, n2), dtype=np.float64)\n",
    "        \n",
    "        for i in prange(n1):\n",
    "            lat1 = np.radians(coords1[i, 0])\n",
    "            lon1 = np.radians(coords1[i, 1])\n",
    "            \n",
    "            for j in range(n2):\n",
    "                if coords2 is None:\n",
    "                    lat2 = np.radians(coords1[j, 0])\n",
    "                    lon2 = np.radians(coords1[j, 1])\n",
    "                else:\n",
    "                    lat2 = np.radians(coords2[j, 0])\n",
    "                    lon2 = np.radians(coords2[j, 1])\n",
    "                    \n",
    "                dlat = lat2 - lat1\n",
    "                dlon = lon2 - lon1\n",
    "                \n",
    "                a = (np.sin(dlat/2)**2 + \n",
    "                     np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2)\n",
    "                c = 2 * np.arcsin(np.sqrt(min(1.0, a)))\n",
    "                dists[i, j] = radius * c\n",
    "                \n",
    "        return dists\n",
    "\n",
    "    def haversine_vectorized(self, coords1, coords2=None, use_cache=False):\n",
    "        \"\"\"Optimized distance calculation with caching and Numba acceleration\"\"\"\n",
    "        cache_key = None\n",
    "        if use_cache and coords2 is None:\n",
    "            cache_key = tuple(map(tuple, coords1))\n",
    "            if cache_key in self._distance_cache:\n",
    "                return self._distance_cache[cache_key]\n",
    "        \n",
    "        if coords1.size < 500:  # Use Numba for smaller datasets\n",
    "            result = self.numba_haversine(coords1, coords2, self.earth_radius_km)\n",
    "        else:\n",
    "            # Use vectorized calculation for larger datasets\n",
    "            if coords2 is None:\n",
    "                coords_rad = np.radians(coords1)\n",
    "                lat = coords_rad[:, 0]\n",
    "                lon = coords_rad[:, 1]\n",
    "                dlat = lat[:, None] - lat\n",
    "                dlon = lon[:, None] - lon\n",
    "                \n",
    "                a = (np.sin(dlat/2)**2 + \n",
    "                     np.cos(lat[:, None]) * np.cos(lat) * np.sin(dlon/2)**2)\n",
    "            else:\n",
    "                coords1_rad = np.radians(coords1)\n",
    "                coords2_rad = np.radians(coords2)\n",
    "                dlat = coords1_rad[:, 0, None] - coords2_rad[:, 0]\n",
    "                dlon = coords1_rad[:, 1, None] - coords2_rad[:, 1]\n",
    "                \n",
    "                a = (np.sin(dlat/2)**2 + \n",
    "                     np.cos(coords1_rad[:, 0, None]) * \n",
    "                     np.cos(coords2_rad[:, 0]) * \n",
    "                     np.sin(dlon/2)**2)\n",
    "            \n",
    "            result = self.earth_radius_km * 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "        \n",
    "        if cache_key is not None:\n",
    "            self._distance_cache[cache_key] = result\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def haversine_single_pair(self, coord1, coord2):\n",
    "        \"\"\"Calculate haversine distance between two points.\"\"\"\n",
    "        lat1, lon1 = np.radians(coord1)\n",
    "        lat2, lon2 = np.radians(coord2)\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = (np.sin(dlat / 2) ** 2 + \n",
    "             np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "        c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "        \n",
    "        return self.earth_radius_km * c \n",
    "\n",
    "    def haversine_pdist(self, coords):\n",
    "        \"\"\"Calculate pairwise distances using scipy format\"\"\"\n",
    "        n = len(coords)\n",
    "        distances = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                dist = self.haversine_single_pair(coords[i], coords[j])\n",
    "                distances.append(dist)\n",
    "        return np.array(distances)\n",
    "\n",
    "    def calculate_cluster_diameter_fast(self, coords):\n",
    "        \"\"\"Optimized diameter calculation with adaptive strategy\"\"\"\n",
    "        n = len(coords)\n",
    "        if n <= 1: \n",
    "            return 0\n",
    "        if n == 2:\n",
    "            return self.haversine_single_pair(coords[0], coords[1])\n",
    "        \n",
    "        # Use convex hull approximation for medium clusters\n",
    "        if n <= 100:\n",
    "            return self._convex_hull_diameter(coords)\n",
    "            \n",
    "        # Use grid-based sampling for large clusters\n",
    "        return self._grid_based_diameter(coords)\n",
    "\n",
    "    def _convex_hull_diameter(self, coords):\n",
    "        \"\"\"Calculate diameter using convex hull approximation\"\"\"\n",
    "        try:\n",
    "            from scipy.spatial import ConvexHull\n",
    "            if len(coords) < 3:\n",
    "                return max(self.haversine_single_pair(coords[i], coords[j]) \n",
    "                          for i in range(len(coords)) for j in range(i+1, len(coords)))\n",
    "            \n",
    "            hull = ConvexHull(coords)\n",
    "            hull_points = coords[hull.vertices]\n",
    "            \n",
    "            max_distance = 0\n",
    "            for i in range(len(hull_points)):\n",
    "                for j in range(i + 1, len(hull_points)):\n",
    "                    distance = self.haversine_single_pair(hull_points[i], hull_points[j])\n",
    "                    max_distance = max(max_distance, distance)\n",
    "            \n",
    "            return max_distance\n",
    "        except:\n",
    "            # Fallback to brute force for small clusters\n",
    "            return self._brute_force_diameter(coords)\n",
    "\n",
    "    def _grid_based_diameter(self, coords):\n",
    "        \"\"\"Use grid-based sampling for large clusters\"\"\"\n",
    "        n_sample = min(50, len(coords))\n",
    "        sample_indices = np.random.choice(len(coords), n_sample, replace=False)\n",
    "        sample_coords = coords[sample_indices]\n",
    "        return self._brute_force_diameter(sample_coords)\n",
    "\n",
    "    def _brute_force_diameter(self, coords):\n",
    "        \"\"\"Calculate exact diameter using brute force\"\"\"\n",
    "        max_distance = 0\n",
    "        n = len(coords)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                distance = self.haversine_single_pair(coords[i], coords[j])\n",
    "                max_distance = max(max_distance, distance)\n",
    "        return max_distance\n",
    "\n",
    "    def should_split_cluster(self, cluster_indices, coords_array):\n",
    "        \"\"\"Determine if a cluster should be split\"\"\"\n",
    "        n_points = len(cluster_indices)\n",
    "        \n",
    "        # Size constraint\n",
    "        if n_points <= self.max_customers_per_cluster:\n",
    "            return False\n",
    "        \n",
    "        # Distance constraint\n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        diameter = self.calculate_cluster_diameter_fast(cluster_coords)\n",
    "        \n",
    "        if diameter <= self.max_distance_km:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def geographic_split(self, cluster_indices, coords_array):\n",
    "        \"\"\"Enhanced splitting with DBSCAN for outlier handling\"\"\"\n",
    "        n_points = len(cluster_indices)\n",
    "        if n_points <= 2:\n",
    "            return [cluster_indices]\n",
    "            \n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        \n",
    "        # Handle outliers with DBSCAN for large clusters\n",
    "        if n_points > 100:\n",
    "            try:\n",
    "                # Create distance matrix for DBSCAN\n",
    "                distances = self.haversine_vectorized(cluster_coords)\n",
    "                dbscan = DBSCAN(eps=self.max_distance_km/2, min_samples=3, \n",
    "                               metric='precomputed')\n",
    "                labels = dbscan.fit_predict(distances)\n",
    "                \n",
    "                if len(np.unique(labels[labels != -1])) > 1:\n",
    "                    return self._balance_split(cluster_indices, labels)\n",
    "            except:\n",
    "                pass  # Fall back to other methods\n",
    "        \n",
    "        # Small clusters: exact method\n",
    "        if n_points <= 50:\n",
    "            return self._exact_geographic_split(cluster_indices, cluster_coords)\n",
    "            \n",
    "        # Medium clusters: k-means with improved initialization\n",
    "        if n_points <= 200:\n",
    "            return self._kmeans_geographic_split(cluster_indices, cluster_coords)\n",
    "            \n",
    "        # Large clusters: hierarchical with complete linkage\n",
    "        return self._hierarchical_geographic_split(cluster_indices, cluster_coords)\n",
    "\n",
    "    def _exact_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Exact splitting for small clusters\"\"\"\n",
    "        if len(cluster_coords) <= 2:\n",
    "            return [cluster_indices]\n",
    "        \n",
    "        # Find the two points that are farthest apart\n",
    "        max_distance = 0\n",
    "        best_pair = (0, 1)\n",
    "        \n",
    "        for i in range(len(cluster_coords)):\n",
    "            for j in range(i + 1, len(cluster_coords)):\n",
    "                distance = self.haversine_single_pair(cluster_coords[i], cluster_coords[j])\n",
    "                if distance > max_distance:\n",
    "                    max_distance = distance\n",
    "                    best_pair = (i, j)\n",
    "        \n",
    "        # Assign points to the closer of the two centers\n",
    "        center1 = cluster_coords[best_pair[0]]\n",
    "        center2 = cluster_coords[best_pair[1]]\n",
    "        \n",
    "        labels = np.zeros(len(cluster_coords))\n",
    "        for i, coord in enumerate(cluster_coords):\n",
    "            dist1 = self.haversine_single_pair(coord, center1)\n",
    "            dist2 = self.haversine_single_pair(coord, center2)\n",
    "            labels[i] = 0 if dist1 <= dist2 else 1\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "\n",
    "    def _kmeans_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"K-means splitting for medium clusters\"\"\"\n",
    "        try:\n",
    "            # Use geographic coordinates directly for K-means\n",
    "            kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(cluster_coords)\n",
    "            return self._balance_split(cluster_indices, labels)\n",
    "        except:\n",
    "            # Fallback to exact method\n",
    "            return self._exact_geographic_split(cluster_indices, cluster_coords)\n",
    "\n",
    "    def _hierarchical_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Improved hierarchical splitting with complete linkage\"\"\"\n",
    "        n_sample = min(150, len(cluster_coords))\n",
    "        sample_indices = np.random.choice(len(cluster_coords), n_sample, replace=False)\n",
    "        sample_coords = cluster_coords[sample_indices]\n",
    "        \n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        linkage_matrix = linkage(distances, method='complete')\n",
    "        sample_labels = fcluster(linkage_matrix, 2, criterion='maxclust') - 1\n",
    "        \n",
    "        # Assign based on nearest cluster center\n",
    "        center1 = np.mean(sample_coords[sample_labels == 0], axis=0)\n",
    "        center2 = np.mean(sample_coords[sample_labels == 1], axis=0)\n",
    "        \n",
    "        dist_to_center1 = self.haversine_vectorized(cluster_coords, center1.reshape(1, -1))[:, 0]\n",
    "        dist_to_center2 = self.haversine_vectorized(cluster_coords, center2.reshape(1, -1))[:, 0]\n",
    "        labels = (dist_to_center1 <= dist_to_center2).astype(int)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "\n",
    "    def _balance_split(self, cluster_indices, labels):\n",
    "        \"\"\"Proximity-based balancing for spatial coherence\"\"\"\n",
    "        unique_labels = np.unique(labels)\n",
    "        if len(unique_labels) == 1:\n",
    "            # All points have same label, split arbitrarily\n",
    "            mid = len(cluster_indices) // 2\n",
    "            return [cluster_indices[:mid], cluster_indices[mid:]]\n",
    "        \n",
    "        # Handle noise points from DBSCAN (label -1)\n",
    "        if -1 in unique_labels:\n",
    "            noise_mask = labels == -1\n",
    "            valid_labels = labels[~noise_mask]\n",
    "            if len(np.unique(valid_labels)) == 0:\n",
    "                return [cluster_indices]\n",
    "            \n",
    "            # Assign noise points to nearest valid cluster\n",
    "            for i in np.where(noise_mask)[0]:\n",
    "                # Find nearest non-noise point\n",
    "                distances = []\n",
    "                for j in np.where(~noise_mask)[0]:\n",
    "                    dist = self.haversine_single_pair(\n",
    "                        self.full_coords_array[cluster_indices[i]], \n",
    "                        self.full_coords_array[cluster_indices[j]]\n",
    "                    )\n",
    "                    distances.append((dist, labels[j]))\n",
    "                \n",
    "                if distances:\n",
    "                    labels[i] = min(distances, key=lambda x: x[0])[1]\n",
    "        \n",
    "        # Create clusters based on labels\n",
    "        clusters = []\n",
    "        for label in np.unique(labels):\n",
    "            if label != -1:  # Skip noise label\n",
    "                cluster_mask = labels == label\n",
    "                cluster = cluster_indices[cluster_mask]\n",
    "                if len(cluster) > 0:\n",
    "                    clusters.append(cluster)\n",
    "        \n",
    "        if len(clusters) == 0:\n",
    "            return [cluster_indices]\n",
    "        elif len(clusters) == 1:\n",
    "            # Split the single cluster arbitrarily\n",
    "            cluster = clusters[0]\n",
    "            mid = len(cluster) // 2\n",
    "            return [cluster[:mid], cluster[mid:]]\n",
    "        else:\n",
    "            # Balance clusters if enabled\n",
    "            if self.balance_clusters and len(clusters) == 2:\n",
    "                return self._balance_two_clusters(clusters[0], clusters[1])\n",
    "            return clusters\n",
    "\n",
    "    def _balance_two_clusters(self, cluster_0, cluster_1):\n",
    "        \"\"\"Balance two clusters by size\"\"\"\n",
    "        size0, size1 = len(cluster_0), len(cluster_1)\n",
    "        \n",
    "        if size0 <= 2 * size1 and size1 <= 2 * size0:\n",
    "            return [cluster_0, cluster_1]\n",
    "        \n",
    "        coords0 = self.full_coords_array[cluster_0]\n",
    "        coords1 = self.full_coords_array[cluster_1]\n",
    "        \n",
    "        # Balance only when size difference is significant\n",
    "        if size0 > 2 * size1:\n",
    "            center1 = np.mean(coords1, axis=0)\n",
    "            dist_to_center1 = self.haversine_vectorized(coords0, center1.reshape(1, -1))[:, 0]\n",
    "            n_move = min((size0 - size1) // 2, max(1, size1))\n",
    "            move_idx = np.argsort(dist_to_center1)[:n_move]\n",
    "            cluster_1 = np.concatenate([cluster_1, cluster_0[move_idx]])\n",
    "            cluster_0 = np.delete(cluster_0, move_idx)\n",
    "            \n",
    "        elif size1 > 2 * size0:\n",
    "            center0 = np.mean(coords0, axis=0)\n",
    "            dist_to_center0 = self.haversine_vectorized(coords1, center0.reshape(1, -1))[:, 0]\n",
    "            n_move = min((size1 - size0) // 2, max(1, size0))\n",
    "            move_idx = np.argsort(dist_to_center0)[:n_move]\n",
    "            cluster_0 = np.concatenate([cluster_0, cluster_1[move_idx]])\n",
    "            cluster_1 = np.delete(cluster_1, move_idx)\n",
    "            \n",
    "        return [cluster_0, cluster_1]\n",
    "\n",
    "    def divisive_clustering(self, customers_df):\n",
    "        \"\"\"Main clustering with cache management\"\"\" \n",
    "        customers_df = customers_df.copy().reset_index(drop=True)\n",
    "        \n",
    "        # Validate input\n",
    "        if 'Latitude' not in customers_df.columns or 'Longitude' not in customers_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'Latitude' and 'Longitude' columns\")\n",
    "        \n",
    "        coords_array = customers_df[['Latitude', 'Longitude']].values\n",
    "        n_customers = len(customers_df)\n",
    "        \n",
    "        self.full_coords_array = coords_array\n",
    "        self._distance_cache = {}  # Reset cache\n",
    "        \n",
    "        if n_customers == 0:\n",
    "            customers_df['cluster'] = []\n",
    "            return customers_df\n",
    "        \n",
    "        if n_customers == 1:\n",
    "            customers_df['cluster'] = 1\n",
    "            return customers_df\n",
    "        \n",
    "        # Priority queue approach for better clustering\n",
    "        clusters_to_process = [(n_customers, np.arange(n_customers))]  # (size, indices)\n",
    "        final_clusters = []\n",
    "        \n",
    "        iteration_count = 0\n",
    "        max_iterations = n_customers * 2\n",
    "        \n",
    "        while clusters_to_process and iteration_count < max_iterations:\n",
    "            # Process largest cluster first\n",
    "            clusters_to_process.sort(key=lambda x: x[0], reverse=True)\n",
    "            current_size, current_cluster_indices = clusters_to_process.pop(0)\n",
    "            iteration_count += 1\n",
    "            \n",
    "            if self.should_split_cluster(current_cluster_indices, coords_array):\n",
    "                subclusters = self.geographic_split(current_cluster_indices, coords_array)\n",
    "                \n",
    "                for subcluster_indices in subclusters:\n",
    "                    if len(subcluster_indices) > 0:\n",
    "                        clusters_to_process.append((len(subcluster_indices), subcluster_indices))\n",
    "            else:\n",
    "                final_clusters.append(current_cluster_indices)\n",
    "        \n",
    "        # Handle remaining clusters\n",
    "        final_clusters.extend([indices for _, indices in clusters_to_process])\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result_df = customers_df.copy()\n",
    "        result_df['cluster'] = -1\n",
    "        \n",
    "        for cluster_id, cluster_indices in enumerate(final_clusters, 1):\n",
    "            result_df.loc[cluster_indices, 'cluster'] = cluster_id\n",
    "        \n",
    "        # Clean up stored data\n",
    "        if hasattr(self, 'full_coords_array'):\n",
    "            del self.full_coords_array\n",
    "        self._distance_cache = {}\n",
    "\n",
    "        return result_df\n",
    "    \n",
    "    def get_cluster_stats(self, clustered_df):\n",
    "        \"\"\"Get comprehensive clustering statistics.\"\"\"\n",
    "        if 'cluster' not in clustered_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'cluster' column\")\n",
    "        \n",
    "        stats = {}\n",
    "        cluster_sizes = []\n",
    "        cluster_diameters = []\n",
    "        \n",
    "        for cluster_id in clustered_df['cluster'].unique():\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "                \n",
    "            cluster_data = clustered_df[clustered_df['cluster'] == cluster_id]\n",
    "            coords = cluster_data[['Latitude', 'Longitude']].values\n",
    "            \n",
    "            diameter = self.calculate_cluster_diameter_fast(coords)\n",
    "            cluster_sizes.append(len(cluster_data))\n",
    "            cluster_diameters.append(diameter)\n",
    "            \n",
    "            stats[cluster_id] = {\n",
    "                'size': len(cluster_data),\n",
    "                'diameter_km': diameter,\n",
    "                'centroid_lat': np.mean(coords[:, 0]),\n",
    "                'centroid_lon': np.mean(coords[:, 1]),\n",
    "                'meets_size_constraint': len(cluster_data) <= self.max_customers_per_cluster,\n",
    "                'meets_distance_constraint': diameter <= self.max_distance_km\n",
    "            }\n",
    "        \n",
    "        # Overall statistics\n",
    "        if cluster_sizes:  # Only calculate if there are clusters\n",
    "            stats['summary'] = {\n",
    "                'total_clusters': len(stats) - 1,  # Excluding summary key\n",
    "                'avg_cluster_size': np.mean(cluster_sizes),\n",
    "                'max_cluster_size': np.max(cluster_sizes),\n",
    "                'min_cluster_size': np.min(cluster_sizes),\n",
    "                'avg_diameter': np.mean(cluster_diameters),\n",
    "                'max_diameter': np.max(cluster_diameters),\n",
    "                'size_violations': sum(1 for size in cluster_sizes if size > self.max_customers_per_cluster),\n",
    "                'distance_violations': sum(1 for diameter in cluster_diameters if diameter > self.max_distance_km)\n",
    "            }\n",
    "        else:\n",
    "            stats['summary'] = {\n",
    "                'total_clusters': 0,\n",
    "                'avg_cluster_size': 0,\n",
    "                'max_cluster_size': 0,\n",
    "                'min_cluster_size': 0,\n",
    "                'avg_diameter': 0,\n",
    "                'max_diameter': 0,\n",
    "                'size_violations': 0,\n",
    "                'distance_violations': 0\n",
    "            }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98368b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numba\n",
    "\n",
    "### Fastest\n",
    "class OptimizedDivisiveGeographicClustering_fast:\n",
    "    def __init__(self, max_customers_per_cluster=20, max_distance_km=50):\n",
    "        self.max_customers_per_cluster = max_customers_per_cluster\n",
    "        self.max_distance_km = max_distance_km\n",
    "        self.earth_radius_km = 6371.0\n",
    "\n",
    "    @staticmethod\n",
    "    @numba.njit(fastmath=True)\n",
    "    def haversine_single_pair(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Numba-optimized haversine distance between two points.\"\"\"\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "        return 6371.0 * 2 * np.arcsin(np.sqrt(min(1.0, a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    @numba.njit(fastmath=True, parallel=True)\n",
    "    def haversine_vectorized(coord, coords):\n",
    "        \"\"\"Vectorized haversine distance from one point to many.\"\"\"\n",
    "        lat1, lon1 = np.radians(coord)\n",
    "        dists = np.empty(coords.shape[0])\n",
    "        for i in numba.prange(coords.shape[0]):\n",
    "            lat2, lon2 = np.radians(coords[i])\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "            dists[i] = 6371.0 * 2 * np.arcsin(np.sqrt(min(1.0, a)))\n",
    "        return dists\n",
    "\n",
    "    def compute_diameter_and_farthest_pair(self, coords):\n",
    "        \"\"\"Compute diameter and farthest pair with optimal strategy.\"\"\"\n",
    "        n = len(coords)\n",
    "        if n <= 100:  # Exact for small clusters\n",
    "            dists = self.haversine_pdist(coords)\n",
    "            dist_matrix = squareform(dists)\n",
    "            max_idx = np.unravel_index(np.argmax(dist_matrix), dist_matrix.shape)\n",
    "            return dist_matrix[max_idx], max_idx\n",
    "        else:  # Approximate for large clusters\n",
    "            return self._two_pass_approximation(coords)\n",
    "\n",
    "    def _two_pass_approximation(self, coords):\n",
    "        \"\"\"Two-pass algorithm for approximate diameter and farthest pair.\"\"\"\n",
    "        # First pass: random point to farthest point\n",
    "        idx0 = np.random.randint(len(coords))\n",
    "        dists = self.haversine_vectorized(coords[idx0], coords)\n",
    "        idx1 = np.argmax(dists)\n",
    "        \n",
    "        # Second pass: farthest point to its farthest point\n",
    "        dists = self.haversine_vectorized(coords[idx1], coords)\n",
    "        idx2 = np.argmax(dists)\n",
    "        diameter = dists[idx2]\n",
    "        \n",
    "        return diameter, (idx1, idx2)\n",
    "\n",
    "    def haversine_pdist(self, coords):\n",
    "        \"\"\"Optimized haversine pairwise distances using vectorization.\"\"\"\n",
    "        n = coords.shape[0]\n",
    "        dists = np.zeros(n*(n-1)//2)\n",
    "        k = 0\n",
    "        for i in range(n):\n",
    "            dists_i = self.haversine_vectorized(coords[i], coords[i+1:])\n",
    "            dists[k:k+len(dists_i)] = dists_i\n",
    "            k += len(dists_i)\n",
    "        return dists\n",
    "\n",
    "    def should_split_cluster(self, cluster_indices, coords_array):\n",
    "        \"\"\"Determine if cluster should be split with early termination.\"\"\"\n",
    "        cluster_size = len(cluster_indices)\n",
    "        \n",
    "        if cluster_size <= 1:\n",
    "            return False, None\n",
    "            \n",
    "        if cluster_size > self.max_customers_per_cluster:\n",
    "            return True, None  # Size violation\n",
    "        \n",
    "        # Check diameter constraint\n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        diameter, farthest_pair = self.compute_diameter_and_farthest_pair(cluster_coords)\n",
    "        return diameter > self.max_distance_km, farthest_pair\n",
    "\n",
    "    def geographic_split(self, cluster_indices, coords_array, farthest_pair=None):\n",
    "        \"\"\"Efficient cluster splitting with optional precomputed centers.\"\"\"\n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        n = len(cluster_coords)\n",
    "        \n",
    "        # Get or compute farthest pair\n",
    "        if farthest_pair is None:\n",
    "            if n <= 100:\n",
    "                _, farthest_pair = self.compute_diameter_and_farthest_pair(cluster_coords)\n",
    "            else:\n",
    "                _, farthest_pair = self._two_pass_approximation(cluster_coords)\n",
    "        \n",
    "        center1_idx, center2_idx = farthest_pair\n",
    "        center1 = cluster_coords[center1_idx]\n",
    "        center2 = cluster_coords[center2_idx]\n",
    "        \n",
    "        # Vectorized distance calculations\n",
    "        dists1 = self.haversine_vectorized(center1, cluster_coords)\n",
    "        dists2 = self.haversine_vectorized(center2, cluster_coords)\n",
    "        labels = (dists1 <= dists2).astype(int)\n",
    "        \n",
    "        # Create subclusters\n",
    "        mask = labels.astype(bool)\n",
    "        cluster_a = cluster_indices[mask]\n",
    "        cluster_b = cluster_indices[~mask]\n",
    "        \n",
    "        # Balance clusters if needed\n",
    "        if len(cluster_a) == 0 or len(cluster_b) == 0:\n",
    "            return self._split_fallback(cluster_indices, cluster_coords)\n",
    "            \n",
    "        return [cluster_a, cluster_b]\n",
    "\n",
    "    def _split_fallback(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Fallback split when primary method fails.\"\"\"\n",
    "        # Use longitude-based split as fallback\n",
    "        sorted_idx = np.argsort(cluster_coords[:, 1])\n",
    "        mid = len(sorted_idx) // 2\n",
    "        return [\n",
    "            cluster_indices[sorted_idx[:mid]],\n",
    "            cluster_indices[sorted_idx[mid:]]\n",
    "        ]\n",
    "\n",
    "    def divisive_clustering(self, customers_df):\n",
    "        \"\"\"Optimized divisive clustering with efficient diameter checks.\"\"\"\n",
    "        # Initialization and validation\n",
    "        if 'Latitude' not in customers_df or 'Longitude' not in customers_df:\n",
    "            raise ValueError(\"Missing Latitude/Longitude columns\")\n",
    "            \n",
    "        coords_array = customers_df[['Latitude', 'Longitude']].values\n",
    "        n = len(customers_df)\n",
    "        \n",
    "        # Edge cases\n",
    "        if n == 0:\n",
    "            return customers_df.assign(cluster=pd.Series(dtype=int))\n",
    "        if n == 1:\n",
    "            return customers_df.assign(cluster=1)\n",
    "        \n",
    "        # Initialize clustering\n",
    "        clusters_to_process = [np.arange(n)]\n",
    "        final_clusters = []\n",
    "        \n",
    "        # Process clusters\n",
    "        while clusters_to_process:\n",
    "            current = clusters_to_process.pop(0)\n",
    "            should_split, farthest_pair = self.should_split_cluster(current, coords_array)\n",
    "            \n",
    "            if should_split:\n",
    "                subclusters = self.geographic_split(\n",
    "                    current, coords_array, farthest_pair\n",
    "                )\n",
    "                clusters_to_process.extend(subclusters)\n",
    "            else:\n",
    "                final_clusters.append(current)\n",
    "        \n",
    "        # Assign cluster labels\n",
    "        cluster_labels = np.zeros(n, dtype=int)\n",
    "        for idx, cluster in enumerate(final_clusters, 1):\n",
    "            cluster_labels[cluster] = idx\n",
    "            \n",
    "        return customers_df.assign(cluster=cluster_labels)\n",
    "\n",
    "    def get_cluster_stats(self, clustered_df):\n",
    "        \"\"\"Get comprehensive clustering statistics.\"\"\"\n",
    "        if 'cluster' not in clustered_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'cluster' column\")\n",
    "        \n",
    "        stats = {}\n",
    "        cluster_sizes = []\n",
    "        cluster_diameters = []\n",
    "        \n",
    "        for cluster_id in clustered_df['cluster'].unique():\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "                \n",
    "            cluster_data = clustered_df[clustered_df['cluster'] == cluster_id]\n",
    "            coords = cluster_data[['Latitude', 'Longitude']].values\n",
    "            n_points = len(coords)\n",
    "            \n",
    "            # Handle diameter calculation efficiently\n",
    "            if n_points == 0:\n",
    "                diameter = 0.0\n",
    "            elif n_points == 1:\n",
    "                diameter = 0.0\n",
    "            else:\n",
    "                diameter, _ = self.compute_diameter_and_farthest_pair(coords)\n",
    "            \n",
    "            cluster_sizes.append(n_points)\n",
    "            cluster_diameters.append(diameter)\n",
    "            \n",
    "            stats[cluster_id] = {\n",
    "                'size': n_points,\n",
    "                'diameter_km': diameter,\n",
    "                'centroid_lat': np.mean(coords[:, 0]) if n_points > 0 else None,\n",
    "                'centroid_lon': np.mean(coords[:, 1]) if n_points > 0 else None,\n",
    "                'meets_size_constraint': n_points <= self.max_customers_per_cluster,\n",
    "                'meets_distance_constraint': diameter <= self.max_distance_km\n",
    "            }\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        total_clusters = len(stats)\n",
    "        size_violations = sum(1 for size in cluster_sizes if size > self.max_customers_per_cluster)\n",
    "        distance_violations = sum(1 for d in cluster_diameters if d > self.max_distance_km)\n",
    "        \n",
    "        # Handle empty case\n",
    "        summary = {\n",
    "            'total_clusters': total_clusters,\n",
    "            'size_violations': size_violations,\n",
    "            'distance_violations': distance_violations,\n",
    "        }\n",
    "        \n",
    "        # Add statistical measures only if clusters exist\n",
    "        if cluster_sizes:\n",
    "            summary.update({\n",
    "                'avg_cluster_size': np.mean(cluster_sizes),\n",
    "                'max_cluster_size': np.max(cluster_sizes),\n",
    "                'min_cluster_size': np.min(cluster_sizes),\n",
    "                'avg_diameter': np.mean(cluster_diameters),\n",
    "                'max_diameter': np.max(cluster_diameters),\n",
    "            })\n",
    "        else:\n",
    "            summary.update({\n",
    "                'avg_cluster_size': 0,\n",
    "                'max_cluster_size': 0,\n",
    "                'min_cluster_size': 0,\n",
    "                'avg_diameter': 0,\n",
    "                'max_diameter': 0,\n",
    "            })\n",
    "        \n",
    "        stats['summary'] = summary\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f978e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. DIVISIVE HIERARCHICAL CLUSTERING\n",
    "# =============================================================================\n",
    "\n",
    "from clustering.divisive_clustering import DivisiveGeographicClustering, OptimizedDivisiveGeographicClustering\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. DIVISIVE HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# divisive_clusterer = OptimizedDivisiveGeographicClustering( # Rivers: Too Long --\n",
    "#     max_customers_per_cluster=20,  # REQUIRED\n",
    "#     max_distance_km=5            # REQUIRED  \n",
    "# ) \n",
    "\n",
    "divisive_clusterer = OptimizedDivisiveGeographicClustering_b( \n",
    "    # Rivers: Divisive clusters created: 48 || Silhouette Score: 0.54 || Constraint violations: Size=6, Distance=3\n",
    "    max_customers_per_cluster=20,  # REQUIRED\n",
    "    max_distance_km=5            # REQUIRED\n",
    "    ,use_vectorized_distances=True, balance_clusters=False\n",
    ")\n",
    "\n",
    "# divisive_clusterer = OptimizedDivisiveGeographicClustering_be( \n",
    "#     # Rivers: Divisive clusters created: 26 || Silhouette Score: 0.57 || Constraint violations: Size=9, Distance=6\n",
    "#     max_customers_per_cluster=20,  # REQUIRED\n",
    "#     max_distance_km=10            # REQUIRED\n",
    "#     ,use_vectorized_distances=True, balance_clusters=False\n",
    "# )\n",
    "\n",
    "# divisive_clusterer = OptimizedDivisiveGeographicClustering_bo( \n",
    "# # Rivers: Divisive clusters created: 60 || Silhouette Score: 0.58 || Constraint violations: Size=9, Distance=7\n",
    "#     max_customers_per_cluster=20,  # REQUIRED\n",
    "#     max_distance_km=5            # REQUIRED \n",
    "# ) \n",
    "\n",
    "# divisive_clusterer = OptimizedDivisiveGeographicClustering_fast(\n",
    "#     # Rivers: Divisive clusters created: 63 || Silhouette Score: 0.46 || Constraint violations: Size=0, Distance=0\n",
    "#     max_customers_per_cluster=20,  # REQUIRED\n",
    "#     max_distance_km=5            # REQUIRED\n",
    "# )\n",
    "\n",
    "# divisive_clusterer = OptimizedDivisiveGeographicClustering_Main( \n",
    "#     # Rivers: Divisive clusters created: 69 ||Silhouette Score: 0.41\n",
    "#     max_customers_per_cluster=20,  # REQUIRED\n",
    "#     max_distance_km=5            # REQUIRED \n",
    "# ) \n",
    "\n",
    "divisive_result = divisive_clusterer.divisive_clustering(df_all_cluster.copy())\n",
    "print(f\"\\nDivisive clusters created: {divisive_result['cluster'].nunique()}\")\n",
    "# print(f\"Cluster sizes: {divisive_result['cluster'].value_counts().sort_index().head()}\")\n",
    "_ = evaluate_unsupervised_clustering(divisive_result)\n",
    "\n",
    "# # Get detailed statistics\n",
    "stats = divisive_clusterer.get_cluster_stats(divisive_result)\n",
    "print(f\"Total clusters: {stats['summary']['total_clusters']}\")\n",
    "print(f\"Constraint violations: Size={stats['summary']['size_violations']}, Distance={stats['summary']['distance_violations']}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisive_result.cluster.value_counts().reset_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd968c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_and_save(df_routes = divisive_result,\n",
    "                 df_stockpoint = None,   \n",
    "                 filename=None,\n",
    "                 cluster_col = 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BaseGeographicClustering:\n",
    "    \"\"\"\n",
    "    A base class containing common geographic utility methods\n",
    "    used by both Divisive and Agglomerative clustering implementations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.earth_radius_km = 6371.0\n",
    "\n",
    "    def haversine_vectorized(self, coords1, coords2=None):\n",
    "        \"\"\"\n",
    "        Highly optimized vectorized haversine distance calculation.\n",
    "        If coords2 is None, calculates pairwise distances within coords1.\n",
    "        Assumes coords are [latitude, longitude].\n",
    "        \"\"\"\n",
    "        if coords2 is None:\n",
    "            # Pairwise distances within coords1 (NxN matrix)\n",
    "            coords1_rad = np.radians(coords1)\n",
    "            \n",
    "            lat1 = coords1_rad[:, 0]\n",
    "            lon1 = coords1_rad[:, 1]\n",
    "            \n",
    "            lat1_mesh, lat2_mesh = np.meshgrid(lat1, lat1)\n",
    "            lon1_mesh, lon2_mesh = np.meshgrid(lon1, lon1)\n",
    "            \n",
    "            dlat = lat2_mesh - lat1_mesh\n",
    "            dlon = lon2_mesh - lon1_mesh\n",
    "            \n",
    "            a = (np.sin(dlat / 2) ** 2 + \n",
    "                 np.cos(lat1_mesh) * np.cos(lat2_mesh) * np.sin(dlon / 2) ** 2)\n",
    "            c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "            \n",
    "            return self.earth_radius_km * c\n",
    "        else:\n",
    "            # Distance from each point in coords1 to each point in coords2 (NxM matrix)\n",
    "            coords1_rad = np.radians(coords1)\n",
    "            coords2_rad = np.radians(coords2)\n",
    "            \n",
    "            lat1 = coords1_rad[:, 0][:, np.newaxis]\n",
    "            lon1 = coords1_rad[:, 1][:, np.newaxis]\n",
    "            lat2 = coords2_rad[:, 0][np.newaxis, :]\n",
    "            lon2 = coords2_rad[:, 1][np.newaxis, :]\n",
    "            \n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            \n",
    "            a = (np.sin(dlat / 2) ** 2 + \n",
    "                 np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "            c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "            \n",
    "            return self.earth_radius_km * c\n",
    "    \n",
    "    def haversine_pdist(self, coords):\n",
    "        \"\"\"\n",
    "        Optimized haversine distance calculation for pdist, returning condensed distance matrix.\n",
    "        Assumes coords are [latitude, longitude].\n",
    "        \"\"\"\n",
    "        def haversine_metric(u, v):\n",
    "            lat1, lon1 = np.radians(u)\n",
    "            lat2, lon2 = np.radians(v)\n",
    "            \n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            \n",
    "            a = (np.sin(dlat / 2) ** 2 + \n",
    "                 np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "            c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "            \n",
    "            return self.earth_radius_km * c\n",
    "        \n",
    "        return pdist(coords, metric=haversine_metric)\n",
    "    \n",
    "    def haversine_single_pair(self, coord1, coord2):\n",
    "        \"\"\"Calculate haversine distance between two points.\"\"\"\n",
    "        lat1, lon1 = np.radians(coord1)\n",
    "        lat2, lon2 = np.radians(coord2)\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = (np.sin(dlat / 2) ** 2 + \n",
    "             np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "        c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "        \n",
    "        return self.earth_radius_km * c\n",
    "\n",
    "    def _convex_hull_diameter(self, coords):\n",
    "        \"\"\"Enhanced convex hull approximation for diameter estimation.\"\"\"\n",
    "        lats, lons = coords[:, 0], coords[:, 1]\n",
    "        \n",
    "        # Get extreme points (min/max lat/lon)\n",
    "        extreme_indices = [\n",
    "            np.argmax(lats), np.argmin(lats),\n",
    "            np.argmax(lons), np.argmin(lons)\n",
    "        ]\n",
    "        \n",
    "        # Add points from different quadrants relative to the mean center\n",
    "        lat_center, lon_center = np.mean(lats), np.mean(lons)\n",
    "        \n",
    "        quadrants = [\n",
    "            (lats >= lat_center) & (lons >= lon_center),   # NE\n",
    "            (lats >= lat_center) & (lons < lon_center),    # NW\n",
    "            (lats < lat_center) & (lons >= lon_center),    # SE\n",
    "            (lats < lat_center) & (lons < lon_center)      # SW\n",
    "        ]\n",
    "        \n",
    "        for quadrant in quadrants:\n",
    "            if np.any(quadrant):\n",
    "                quad_indices = np.where(quadrant)[0]\n",
    "                # Add furthest point from center in each non-empty quadrant\n",
    "                distances_from_center = np.sqrt(\n",
    "                    (lats[quad_indices] - lat_center)**2 + \n",
    "                    (lons[quad_indices] - lon_center)**2\n",
    "                )\n",
    "                furthest_idx = quad_indices[np.argmax(distances_from_center)]\n",
    "                extreme_indices.append(furthest_idx)\n",
    "        \n",
    "        # Add some random points to further improve approximation for larger clusters\n",
    "        n_random = min(12, len(coords) - len(set(extreme_indices)))\n",
    "        if n_random > 0:\n",
    "            available_indices = list(set(range(len(coords))) - set(extreme_indices))\n",
    "            if available_indices:\n",
    "                random_indices = np.random.choice(available_indices, \n",
    "                                                  min(n_random, len(available_indices)), \n",
    "                                                  replace=False)\n",
    "                extreme_indices.extend(random_indices)\n",
    "        \n",
    "        # Get unique sample coordinates\n",
    "        sample_indices = list(set(extreme_indices))\n",
    "        sample_coords = coords[sample_indices]\n",
    "        \n",
    "        if len(sample_coords) <= 1:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate max distance among the sampled points\n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        return np.max(distances)\n",
    "    \n",
    "    def _grid_based_diameter(self, coords):\n",
    "        \"\"\"Grid-based sampling for very large clusters.\"\"\"\n",
    "        lats, lons = coords[:, 0], coords[:, 1]\n",
    "        \n",
    "        # Create 6x6 grid\n",
    "        lat_bins = np.linspace(lats.min(), lats.max(), 7)\n",
    "        lon_bins = np.linspace(lons.min(), lons.max(), 7)\n",
    "        \n",
    "        sample_indices = []\n",
    "        for i in range(len(lat_bins)-1):\n",
    "            for j in range(len(lon_bins)-1):\n",
    "                mask = ((lats >= lat_bins[i]) & (lats < lat_bins[i+1]) & \n",
    "                        (lons >= lon_bins[j]) & (lons < lon_bins[j+1]))\n",
    "                cell_indices = np.where(mask)[0]\n",
    "                if len(cell_indices) > 0:\n",
    "                    # Sample up to 2 points from each cell\n",
    "                    n_sample = min(2, len(cell_indices))\n",
    "                    sampled = np.random.choice(cell_indices, n_sample, replace=False)\n",
    "                    sample_indices.extend(sampled)\n",
    "        \n",
    "        if len(sample_indices) <= 1:\n",
    "            return 0\n",
    "        \n",
    "        sample_coords = coords[sample_indices]\n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        return np.max(distances)\n",
    "    \n",
    "    def calculate_cluster_diameter_fast(self, coords):\n",
    "        \"\"\"\n",
    "        Fast cluster diameter calculation with multiple optimization strategies.\n",
    "        \"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        if n_points <= 1:\n",
    "            return 0\n",
    "        \n",
    "        if n_points == 2:\n",
    "            return self.haversine_single_pair(coords[0], coords[1])\n",
    "        \n",
    "        # Use different strategies based on cluster size\n",
    "        if n_points <= 10:\n",
    "            # Small clusters: exact calculation using pdist\n",
    "            distances = self.haversine_pdist(coords)\n",
    "            return np.max(distances)\n",
    "        elif n_points <= 50:\n",
    "            # Medium clusters: vectorized calculation or pdist\n",
    "            # Prioritize vectorized if available and faster for this range\n",
    "            distance_matrix = self.haversine_vectorized(coords)\n",
    "            # Ensure we're not taking max of diagonal (self-distances = 0)\n",
    "            return np.max(distance_matrix[np.triu_indices(n_points, k=1)])\n",
    "        else:\n",
    "            # Large clusters: smart sampling\n",
    "            return self._smart_diameter_estimation(coords)\n",
    "    \n",
    "    def _smart_diameter_estimation(self, coords):\n",
    "        \"\"\"\n",
    "        Improved diameter estimation using multiple sampling strategies.\n",
    "        \"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        # Strategy 1: Convex hull approximation\n",
    "        hull_diameter = self._convex_hull_diameter(coords)\n",
    "        \n",
    "        # Strategy 2: Grid-based sampling for very large clusters (higher confidence for max)\n",
    "        if n_points > 200: # Threshold for when grid sampling might be beneficial\n",
    "            grid_diameter = self._grid_based_diameter(coords)\n",
    "            return max(hull_diameter, grid_diameter)\n",
    "        \n",
    "        return hull_diameter\n",
    "\n",
    "    def _find_approximate_farthest_pair(self, coords):\n",
    "        \"\"\"Find approximate farthest pair for large clusters.\n",
    "           Moved from OptimizedDivisiveGeographicClustering to BaseGeographicClustering.\"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        if n_points <= 100:\n",
    "            # For moderate sizes, use exact calculation\n",
    "            distances = self.haversine_pdist(coords)\n",
    "            distance_matrix = squareform(distances)\n",
    "            max_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "            return max_idx[0], max_idx[1]\n",
    "        \n",
    "        # For large clusters, use sampling\n",
    "        sample_size = min(50, n_points)\n",
    "        sample_indices = np.random.choice(n_points, sample_size, replace=False)\n",
    "        sample_coords = coords[sample_indices]\n",
    "        \n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        distance_matrix = squareform(distances)\n",
    "        max_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "        \n",
    "        return sample_indices[max_idx[0]], sample_indices[max_idx[1]]\n",
    "\n",
    "    def get_cluster_stats(self, clustered_df, max_customers_per_cluster, max_distance_km):\n",
    "        \"\"\"Get comprehensive clustering statistics.\"\"\"\n",
    "        if 'cluster' not in clustered_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'cluster' column\")\n",
    "        \n",
    "        stats = {}\n",
    "        cluster_sizes = []\n",
    "        cluster_diameters = []\n",
    "        \n",
    "        for cluster_id in clustered_df['cluster'].unique():\n",
    "            if cluster_id == -1: # Unassigned points if any\n",
    "                continue\n",
    "            \n",
    "            cluster_data = clustered_df[clustered_df['cluster'] == cluster_id]\n",
    "            coords = cluster_data[['Latitude', 'Longitude']].values\n",
    "            \n",
    "            diameter = self.calculate_cluster_diameter_fast(coords)\n",
    "            cluster_sizes.append(len(cluster_data))\n",
    "            cluster_diameters.append(diameter)\n",
    "            \n",
    "            stats[cluster_id] = {\n",
    "                'size': len(cluster_data),\n",
    "                'diameter_km': diameter,\n",
    "                'centroid_lat': np.mean(coords[:, 0]),\n",
    "                'centroid_lon': np.mean(coords[:, 1]),\n",
    "                'meets_size_constraint': len(cluster_data) <= max_customers_per_cluster,\n",
    "                'meets_distance_constraint': diameter <= max_distance_km\n",
    "            }\n",
    "        \n",
    "        # Overall statistics\n",
    "        stats['summary'] = {\n",
    "            'total_clusters': len(stats), \n",
    "            'avg_cluster_size': np.mean(cluster_sizes) if cluster_sizes else 0,\n",
    "            'max_cluster_size': np.max(cluster_sizes) if cluster_sizes else 0,\n",
    "            'min_cluster_size': np.min(cluster_sizes) if cluster_sizes else 0,\n",
    "            'avg_diameter': np.mean(cluster_diameters) if cluster_diameters else 0,\n",
    "            'max_diameter': np.max(cluster_diameters) if cluster_diameters else 0,\n",
    "            'size_violations': sum(1 for size in cluster_sizes if size > max_customers_per_cluster),\n",
    "            'distance_violations': sum(1 for diameter in cluster_diameters if diameter > max_distance_km)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "class AgglomerativeGeographicClustering(BaseGeographicClustering):\n",
    "    def __init__(self, max_customers_per_cluster=20, max_distance_km=50, \n",
    "                 linkage_method='ward', sub_cluster_if_oversized=True):\n",
    "        \"\"\"\n",
    "        Initializes the Agglomerative Geographic Clustering.\n",
    "\n",
    "        Args:\n",
    "            max_customers_per_cluster (int): Maximum number of customers allowed in a single cluster.\n",
    "            max_distance_km (float): Maximum diameter (distance between two farthest points)\n",
    "                                     allowed within a cluster in kilometers.\n",
    "            linkage_method (str): Method to use for calculating the distance between clusters\n",
    "                                  in hierarchical clustering. Options: 'ward', 'single', 'complete', 'average'.\n",
    "            sub_cluster_if_oversized (bool): If True, clusters that exceed max_customers_per_cluster\n",
    "                                             after distance-based cutting will be further sub-clustered\n",
    "                                             using K-Means.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_customers_per_cluster = max_customers_per_cluster\n",
    "        self.max_distance_km = max_distance_km\n",
    "        self.linkage_method = linkage_method\n",
    "        self.sub_cluster_if_oversized = sub_cluster_if_oversized\n",
    "\n",
    "    def agglomerative_clustering(self, customers_df):\n",
    "        \"\"\"\n",
    "        Performs agglomerative hierarchical clustering on geographic data\n",
    "        with constraints on cluster size and diameter.\n",
    "        \"\"\"\n",
    "        customers_df = customers_df.copy().reset_index(drop=True)\n",
    "        \n",
    "        if 'Latitude' not in customers_df.columns or 'Longitude' not in customers_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'Latitude' and 'Longitude' columns\")\n",
    "        \n",
    "        coords_array = customers_df[['Latitude', 'Longitude']].values\n",
    "        n_customers = len(customers_df)\n",
    "        \n",
    "        if n_customers == 0:\n",
    "            customers_df['cluster'] = []\n",
    "            return customers_df\n",
    "        \n",
    "        if n_customers == 1:\n",
    "            customers_df['cluster'] = 1\n",
    "            return customers_df\n",
    "\n",
    "        # Step 1: Calculate pairwise Haversine distances\n",
    "        print(f\"Calculating {n_customers*(n_customers-1)//2} pairwise distances...\")\n",
    "        # Check if the number of points is too large for pdist to avoid MemoryError\n",
    "        # A rough heuristic: 5000 points * 5000 points / 2 * 8 bytes/float ~ 100MB\n",
    "        # For very large N, consider approximate methods if pdist is too slow/memory intensive\n",
    "        if n_customers > 2000 and self.linkage_method != 'ward': # Ward only works with Euclidean-like pdist\n",
    "             # For very large datasets, pdist might be too slow or memory intensive.\n",
    "             # In such cases, one might consider sampling or approximate hierarchical methods,\n",
    "             # or other clustering algorithms like DBSCAN that don't require a full distance matrix.\n",
    "             # For now, we proceed with pdist as it's standard for scipy.hierarchy.\n",
    "            print(\"Warning: Large dataset for pdist. This might take a while or consume a lot of memory.\")\n",
    "\n",
    "        distances = self.haversine_pdist(coords_array)\n",
    "        \n",
    "        # Step 2: Perform hierarchical clustering using linkage\n",
    "        print(f\"Performing linkage using '{self.linkage_method}' method...\")\n",
    "        linkage_matrix = linkage(distances, method=self.linkage_method)\n",
    "        \n",
    "        # Step 3: Cut the dendrogram based on max_distance_km\n",
    "        # This creates clusters where no two points are farther apart than max_distance_km\n",
    "        print(f\"Cutting dendrogram at max_distance_km={self.max_distance_km}...\")\n",
    "        initial_labels = fcluster(linkage_matrix, self.max_distance_km, criterion='distance')\n",
    "        \n",
    "        customers_df['cluster_temp'] = initial_labels\n",
    "        final_cluster_id = 0\n",
    "        final_clusters = {}\n",
    "\n",
    "        # Step 4: Post-process for max_customers_per_cluster constraint\n",
    "        print(f\"Post-processing clusters for size constraint (max {self.max_customers_per_cluster} customers)...\")\n",
    "        for current_cluster_label in sorted(customers_df['cluster_temp'].unique()):\n",
    "            cluster_indices = customers_df[customers_df['cluster_temp'] == current_cluster_label].index.values\n",
    "            current_coords = coords_array[cluster_indices]\n",
    "            \n",
    "            if len(cluster_indices) > self.max_customers_per_cluster and self.sub_cluster_if_oversized:\n",
    "                print(f\"  Cluster {current_cluster_label} (size {len(cluster_indices)}) is oversized. Sub-clustering...\")\n",
    "                # Sub-cluster using K-Means. Determine optimal k based on current size / max_customers_per_cluster\n",
    "                k_sub = int(np.ceil(len(cluster_indices) / self.max_customers_per_cluster))\n",
    "                k_sub = max(2, k_sub) # Ensure at least 2 clusters if splitting\n",
    "                \n",
    "                # Use approximate farthest pair for K-Means initialization\n",
    "                initial_centers_indices = self._find_approximate_farthest_pair(current_coords)\n",
    "                # Ensure initial_centers_indices has enough elements for k_sub.\n",
    "                # If k_sub > 2, KMeans++ initialization is generally more robust than a simple farthest pair.\n",
    "                if k_sub > 2:\n",
    "                    kmeans_sub = KMeans(n_clusters=k_sub, n_init=10, random_state=42) # Let KMeans find its own init\n",
    "                elif len(initial_centers_indices) >= 2: # k_sub = 2, use the farthest pair if available\n",
    "                    kmeans_sub = KMeans(n_clusters=k_sub, init=current_coords[[initial_centers_indices[0], initial_centers_indices[1]]], n_init=1, random_state=42)\n",
    "                else: # Fallback if initial_centers_indices is not sufficient for k_sub=2\n",
    "                    kmeans_sub = KMeans(n_clusters=k_sub, n_init=10, random_state=42)\n",
    "\n",
    "\n",
    "                sub_labels = kmeans_sub.fit_predict(current_coords)\n",
    "                \n",
    "                for sub_label in np.unique(sub_labels):\n",
    "                    final_cluster_id += 1\n",
    "                    sub_cluster_indices = cluster_indices[sub_labels == sub_label]\n",
    "                    final_clusters[final_cluster_id] = sub_cluster_indices\n",
    "            else:\n",
    "                final_cluster_id += 1\n",
    "                final_clusters[final_cluster_id] = cluster_indices\n",
    "        \n",
    "        # Assign final cluster labels to the DataFrame\n",
    "        result_df = customers_df.copy()\n",
    "        result_df['cluster'] = -1 # Initialize with unassigned\n",
    "\n",
    "        for cluster_id, indices in final_clusters.items():\n",
    "            result_df.loc[indices, 'cluster'] = cluster_id\n",
    "        \n",
    "        result_df = result_df.drop(columns=['cluster_temp'])\n",
    "        print(\"Agglomerative clustering completed.\\n\")\n",
    "        return result_df\n",
    "\n",
    "class OptimizedDivisiveGeographicClustering(BaseGeographicClustering):\n",
    "    \"\"\" Best - Fast and Accurate Divisive Geographic Clustering\n",
    "    \"\"\"\n",
    "    def __init__(self, max_customers_per_cluster=20, max_distance_km=50, \n",
    "                 use_vectorized_distances=True, balance_clusters=False):\n",
    "        super().__init__()\n",
    "        self.max_customers_per_cluster = max_customers_per_cluster\n",
    "        self.max_distance_km = max_distance_km\n",
    "        self.use_vectorized_distances = use_vectorized_distances\n",
    "        self.balance_clusters = balance_clusters\n",
    "        \n",
    "    def calculate_cluster_diameter_fast(self, coords): # Overrides Base class method\n",
    "        \"\"\"\n",
    "        Fast cluster diameter calculation with multiple optimization strategies.\n",
    "        \"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        if n_points <= 1:\n",
    "            return 0\n",
    "        \n",
    "        if n_points == 2:\n",
    "            return self.haversine_single_pair(coords[0], coords[1])\n",
    "        \n",
    "        # Use different strategies based on cluster size\n",
    "        if n_points <= 10:\n",
    "            # Small clusters: exact calculation\n",
    "            distances = self.haversine_pdist(coords)\n",
    "            return np.max(distances)\n",
    "        elif n_points <= 50:\n",
    "            # Medium clusters: vectorized calculation\n",
    "            if self.use_vectorized_distances:\n",
    "                distance_matrix = self.haversine_vectorized(coords)\n",
    "                return np.max(distance_matrix)\n",
    "            else:\n",
    "                distances = self.haversine_pdist(coords)\n",
    "                return np.max(distances)\n",
    "        else:\n",
    "            # Large clusters: smart sampling\n",
    "            return self._smart_diameter_estimation(coords)\n",
    "    \n",
    "    def _smart_diameter_estimation(self, coords): # Overrides Base class method\n",
    "        \"\"\"\n",
    "        Improved diameter estimation using multiple sampling strategies.\n",
    "        \"\"\"\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        # Strategy 1: Convex hull approximation\n",
    "        hull_diameter = self._convex_hull_diameter(coords)\n",
    "        \n",
    "        # Strategy 2: Grid-based sampling for large clusters\n",
    "        if n_points > 200:\n",
    "            grid_diameter = self._grid_based_diameter(coords)\n",
    "            return max(hull_diameter, grid_diameter)\n",
    "        \n",
    "        return hull_diameter\n",
    "    \n",
    "    def should_split_cluster(self, cluster_indices, coords_array):\n",
    "        \"\"\"Enhanced cluster splitting logic with load balancing.\"\"\"\n",
    "        cluster_size = len(cluster_indices)\n",
    "        \n",
    "        if cluster_size <= 2:\n",
    "            return False\n",
    "        \n",
    "        # Hard size constraint\n",
    "        if cluster_size > self.max_customers_per_cluster * 1.5:\n",
    "            return True\n",
    "        \n",
    "        # Soft size constraint with diameter check\n",
    "        if cluster_size > self.max_customers_per_cluster:\n",
    "            cluster_coords = coords_array[cluster_indices]\n",
    "            diameter = self.calculate_cluster_diameter_fast(cluster_coords)\n",
    "            return diameter > self.max_distance_km * 0.8  # More lenient for size\n",
    "        \n",
    "        # Diameter constraint\n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        diameter = self.calculate_cluster_diameter_fast(cluster_coords)\n",
    "        \n",
    "        return diameter > self.max_distance_km\n",
    "    \n",
    "    def geographic_split(self, cluster_indices, coords_array):\n",
    "        \"\"\"\n",
    "        Improved geographic splitting with better load balancing.\n",
    "        \"\"\"\n",
    "        if len(cluster_indices) <= 2:\n",
    "            return [cluster_indices]\n",
    "        \n",
    "        cluster_coords = coords_array[cluster_indices]\n",
    "        n_points = len(cluster_coords)\n",
    "        \n",
    "        # For small clusters, use exact method\n",
    "        if n_points <= 50:\n",
    "            return self._exact_geographic_split(cluster_indices, cluster_coords)\n",
    "        \n",
    "        # For medium clusters, use K-means with geographic initialization\n",
    "        if n_points <= 200:\n",
    "            return self._kmeans_geographic_split(cluster_indices, cluster_coords)\n",
    "        \n",
    "        # For large clusters, use hierarchical approach\n",
    "        return self._hierarchical_geographic_split(cluster_indices, cluster_coords)\n",
    "    \n",
    "    def _exact_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Exact splitting for small clusters.\"\"\"\n",
    "        n_points = len(cluster_coords)\n",
    "        \n",
    "        # Find the two points that are farthest apart\n",
    "        distances = self.haversine_pdist(cluster_coords)\n",
    "        distance_matrix = squareform(distances)\n",
    "        max_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "        center1_idx, center2_idx = max_idx[0], max_idx[1]\n",
    "        \n",
    "        center1 = cluster_coords[center1_idx]\n",
    "        center2 = cluster_coords[center2_idx]\n",
    "        \n",
    "        # Assign points to closest center\n",
    "        distances_to_center1 = self.haversine_vectorized(cluster_coords, center1.reshape(1, -1))[:, 0]\n",
    "        distances_to_center2 = self.haversine_vectorized(cluster_coords, center2.reshape(1, -1))[:, 0]\n",
    "        \n",
    "        labels = (distances_to_center1 <= distances_to_center2).astype(int)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "    \n",
    "    def _kmeans_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"K-means splitting with geographic initialization.\"\"\"\n",
    "        n_points = len(cluster_coords)\n",
    "        \n",
    "        # Initialize with farthest pair\n",
    "        # This calls _find_approximate_farthest_pair from BaseGeographicClustering\n",
    "        center1_idx, center2_idx = self._find_approximate_farthest_pair(cluster_coords) \n",
    "        initial_centers = cluster_coords[[center1_idx, center2_idx]]\n",
    "        \n",
    "        # Apply K-means\n",
    "        kmeans = KMeans(n_clusters=2, init=initial_centers, n_init=1, random_state=42)\n",
    "        labels = kmeans.fit_predict(cluster_coords)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "    \n",
    "    def _hierarchical_geographic_split(self, cluster_indices, cluster_coords):\n",
    "        \"\"\"Hierarchical splitting for large clusters.\"\"\"\n",
    "        # Use linkage-based clustering for very large clusters\n",
    "        n_sample = min(100, len(cluster_coords))\n",
    "        sample_indices = np.random.choice(len(cluster_coords), n_sample, replace=False)\n",
    "        sample_coords = cluster_coords[sample_indices]\n",
    "        \n",
    "        # Compute linkage on sample\n",
    "        distances = self.haversine_pdist(sample_coords)\n",
    "        linkage_matrix = linkage(distances, method='ward')\n",
    "        sample_labels = fcluster(linkage_matrix, 2, criterion='maxclust') - 1\n",
    "        \n",
    "        # Ensure that both sub-clusters have points from the sample\n",
    "        center1_coords = sample_coords[sample_labels == 0]\n",
    "        center2_coords = sample_coords[sample_labels == 1]\n",
    "        \n",
    "        if len(center1_coords) == 0 or len(center2_coords) == 0:\n",
    "            # Fallback to exact split if hierarchical sample leads to empty sub-clusters\n",
    "            return self._exact_geographic_split(cluster_indices, cluster_coords)\n",
    "        \n",
    "        center1 = np.mean(center1_coords, axis=0)\n",
    "        center2 = np.mean(center2_coords, axis=0)\n",
    "        \n",
    "        distances_to_center1 = self.haversine_vectorized(cluster_coords, center1.reshape(1, -1))[:, 0]\n",
    "        distances_to_center2 = self.haversine_vectorized(cluster_coords, center2.reshape(1, -1))[:, 0]\n",
    "        \n",
    "        labels = (distances_to_center1 <= distances_to_center2).astype(int)\n",
    "        \n",
    "        return self._balance_split(cluster_indices, labels)\n",
    "    \n",
    "    def _balance_split(self, cluster_indices, labels):\n",
    "        \"\"\"Balance the split to avoid very uneven clusters.\"\"\"\n",
    "        cluster_0_indices = cluster_indices[labels == 0]\n",
    "        cluster_1_indices = cluster_indices[labels == 1]\n",
    "        \n",
    "        # Ensure no empty clusters (very important for recursive calls)\n",
    "        if len(cluster_0_indices) == 0:\n",
    "            # If one cluster is empty, move one point from the other to it.\n",
    "            # This handles edge cases but might lead to a single point cluster,\n",
    "            # which the should_split_cluster check will prevent further splitting if <= 2.\n",
    "            if len(cluster_1_indices) > 0:\n",
    "                cluster_0_indices = np.array([cluster_1_indices[0]])\n",
    "                cluster_1_indices = cluster_1_indices[1:]\n",
    "            else: # Both empty, should not happen if initial cluster_indices was not empty\n",
    "                return [np.array([]), np.array([])]\n",
    "        elif len(cluster_1_indices) == 0:\n",
    "            if len(cluster_0_indices) > 0:\n",
    "                cluster_1_indices = np.array([cluster_0_indices[0]])\n",
    "                cluster_0_indices = cluster_0_indices[1:]\n",
    "            else: # Both empty\n",
    "                return [np.array([]), np.array([])]\n",
    "        \n",
    "        # Optional: Balance cluster sizes if one is much larger\n",
    "        if self.balance_clusters:\n",
    "            size_0, size_1 = len(cluster_0_indices), len(cluster_1_indices)\n",
    "            if size_0 > 3 * size_1 and size_1 > 0: # If cluster 0 is significantly larger\n",
    "                n_move = (size_0 - size_1) // 4 # Move a quarter of the difference\n",
    "                move_indices = cluster_0_indices[:n_move]\n",
    "                cluster_0_indices = cluster_0_indices[n_move:]\n",
    "                cluster_1_indices = np.concatenate([cluster_1_indices, move_indices])\n",
    "            elif size_1 > 3 * size_0 and size_0 > 0: # If cluster 1 is significantly larger\n",
    "                n_move = (size_1 - size_0) // 4\n",
    "                move_indices = cluster_1_indices[:n_move]\n",
    "                cluster_1_indices = cluster_1_indices[n_move:]\n",
    "                cluster_0_indices = np.concatenate([cluster_0_indices, move_indices])\n",
    "        \n",
    "        return [cluster_0_indices, cluster_1_indices]\n",
    "    \n",
    "    def divisive_clustering(self, customers_df):\n",
    "        \"\"\"Perform optimized divisive hierarchical clustering.\"\"\"\n",
    "        customers_df = customers_df.copy().reset_index(drop=True)\n",
    "        \n",
    "        # Validate input\n",
    "        if 'Latitude' not in customers_df.columns or 'Longitude' not in customers_df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'Latitude' and 'Longitude' columns\")\n",
    "        \n",
    "        coords_array = customers_df[['Latitude', 'Longitude']].values\n",
    "        n_customers = len(customers_df)\n",
    "        \n",
    "        if n_customers == 0:\n",
    "            customers_df['cluster'] = []\n",
    "            return customers_df\n",
    "        \n",
    "        if n_customers == 1:\n",
    "            customers_df['cluster'] = 1\n",
    "            return customers_df\n",
    "        \n",
    "        # Priority queue approach for better clustering\n",
    "        clusters_to_process = [(n_customers, np.arange(n_customers))]   # (size, indices)\n",
    "        final_clusters = []\n",
    "        \n",
    "        iteration_count = 0\n",
    "        max_iterations = n_customers * 2 # Safety break to prevent infinite loops\n",
    "        \n",
    "        while clusters_to_process and iteration_count < max_iterations:\n",
    "            # Process largest cluster first to tackle the biggest problems\n",
    "            clusters_to_process.sort(key=lambda x: x[0], reverse=True)\n",
    "            current_size, current_cluster_indices = clusters_to_process.pop(0)\n",
    "            iteration_count += 1\n",
    "            \n",
    "            if self.should_split_cluster(current_cluster_indices, coords_array):\n",
    "                subclusters = self.geographic_split(current_cluster_indices, coords_array)\n",
    "                \n",
    "                for subcluster_indices in subclusters:\n",
    "                    if len(subcluster_indices) > 0:\n",
    "                        clusters_to_process.append((len(subcluster_indices), subcluster_indices))\n",
    "            else:\n",
    "                final_clusters.append(current_cluster_indices)\n",
    "        \n",
    "        # Handle any clusters remaining in `clusters_to_process` if max_iterations was hit\n",
    "        final_clusters.extend([indices for _, indices in clusters_to_process])\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result_df = customers_df.copy()\n",
    "        result_df['cluster'] = -1 # Initialize with -1 for unassigned\n",
    "        \n",
    "        for cluster_id, cluster_indices in enumerate(final_clusters, 1):\n",
    "            result_df.loc[cluster_indices, 'cluster'] = cluster_id\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c64bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad756b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### My custom testing\n",
    "print(\"--- Running Agglomerative Clustering ---\")\n",
    "agg_clusterer = AgglomerativeGeographicClustering(\n",
    "    max_customers_per_cluster=20, # Aim for clusters of max 50 customers\n",
    "    max_distance_km=5.0,        # Max diameter of 5 km\n",
    "    linkage_method='ward',       # Common choice for compact clusters\n",
    "    sub_cluster_if_oversized=True\n",
    ")\n",
    "clustered_agg_df = agg_clusterer.agglomerative_clustering(df_all_cluster.copy())\n",
    "agg_stats = agg_clusterer.get_cluster_stats(clustered_agg_df, agg_clusterer.max_customers_per_cluster, agg_clusterer.max_distance_km)\n",
    "print(\"\\nAgglomerative Clustering Stats:\")\n",
    "for k, v in agg_stats['summary'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nSample Agglomerative Cluster Details:\")\n",
    "for cluster_id, details in list(agg_stats.items())[:5]: # Show first 5 clusters\n",
    "    if cluster_id != 'summary':\n",
    "        print(f\"  Cluster {cluster_id}: Size={details['size']}, Diameter={details['diameter_km']:.2f} km, Size OK={details['meets_size_constraint']}, Distance OK={details['meets_distance_constraint']}\")\n",
    "\n",
    "\n",
    "_ = evaluate_unsupervised_clustering(clustered_agg_df)\n",
    "\n",
    "# # # Get detailed statistics\n",
    "stats = divisive_clusterer.get_cluster_stats(clustered_agg_df)\n",
    "print(f\"Total clusters: {stats['summary']['total_clusters']}\")\n",
    "print(f\"Constraint violations: Size={stats['summary']['size_violations']}, Distance={stats['summary']['distance_violations']}\")\n",
    " \n",
    "\n",
    " \n",
    "vis_and_save(df_routes = clustered_agg_df,\n",
    "                 df_stockpoint = None,   \n",
    "                 filename=None,\n",
    "                 cluster_col = 'cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bf2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### My custom testing\n",
    "print(\"\\n--- Running Divisive Clustering (for comparison) ---\")\n",
    "div_clusterer = OptimizedDivisiveGeographicClustering(\n",
    "    max_customers_per_cluster=50,\n",
    "    max_distance_km=5.0,\n",
    "    balance_clusters=True\n",
    ")\n",
    "clustered_div_df = div_clusterer.divisive_clustering(df_all_cluster.copy())\n",
    "div_stats = div_clusterer.get_cluster_stats(clustered_div_df, div_clusterer.max_customers_per_cluster, div_clusterer.max_distance_km) # Added missing arguments\n",
    "print(\"\\nDivisive Clustering Stats:\")\n",
    "for k, v in div_stats['summary'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nSample Divisive Cluster Details:\")\n",
    "for cluster_id, details in list(div_stats.items())[:5]: # Show first 5 clusters\n",
    "    if cluster_id != 'summary':\n",
    "        print(f\"  Cluster {cluster_id}: Size={details['size']}, Diameter={details['diameter_km']:.2f} km, Size OK={details['meets_size_constraint']}, Distance OK={details['meets_distance_constraint']}\")\n",
    "\n",
    "\n",
    "_ = evaluate_unsupervised_clustering(clustered_div_df)\n",
    "\n",
    "# # # Get detailed statistics\n",
    "stats = divisive_clusterer.get_cluster_stats(clustered_div_df)\n",
    "print(f\"Total clusters: {stats['summary']['total_clusters']}\")\n",
    "print(f\"Constraint violations: Size={stats['summary']['size_violations']}, Distance={stats['summary']['distance_violations']}\")\n",
    "\n",
    "\n",
    "\n",
    "vis_and_save(df_routes = clustered_div_df,\n",
    "                df_stockpoint = None,   \n",
    "                filename=None,\n",
    "                cluster_col = 'cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c5c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f57385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some sample geographic data\n",
    "    np.random.seed(42)\n",
    "    num_customers = 500 # Testing with more customers for better demonstration\n",
    "    \n",
    "    # Simulate clusters\n",
    "    center1 = [6.5, 3.3] # Lagos area\n",
    "    center2 = [6.6, 3.4]\n",
    "    center3 = [6.4, 3.2]\n",
    "\n",
    "    customers_data = []\n",
    "    # Cluster 1 (dense)\n",
    "    for _ in range(200):\n",
    "        customers_data.append({\n",
    "            'CustomerID': f'C1_{_}',\n",
    "            'Latitude': center1[0] + np.random.randn() * 0.01,\n",
    "            'Longitude': center1[1] + np.random.randn() * 0.01\n",
    "        })\n",
    "    # Cluster 2 (dense)\n",
    "    for _ in range(150):\n",
    "        customers_data.append({\n",
    "            'CustomerID': f'C2_{_}',\n",
    "            'Latitude': center2[0] + np.random.randn() * 0.015,\n",
    "            'Longitude': center2[1] + np.random.randn() * 0.015\n",
    "        })\n",
    "    # Cluster 3 (sparse, might get split or remain single if large enough)\n",
    "    for _ in range(150):\n",
    "        customers_data.append({\n",
    "            'CustomerID': f'C3_{_}',\n",
    "            'Latitude': center3[0] + np.random.randn() * 0.02,\n",
    "            'Longitude': center3[1] + np.random.randn() * 0.02\n",
    "        })\n",
    "\n",
    "    customers_df = pd.DataFrame(customers_data)\n",
    "\n",
    "    print(\"--- Running Agglomerative Clustering ---\")\n",
    "    agg_clusterer = AgglomerativeGeographicClustering(\n",
    "        max_customers_per_cluster=50, # Aim for clusters of max 50 customers\n",
    "        max_distance_km=5.0,        # Max diameter of 5 km\n",
    "        linkage_method='ward',       # Common choice for compact clusters\n",
    "        sub_cluster_if_oversized=True\n",
    "    )\n",
    "    clustered_agg_df = agg_clusterer.agglomerative_clustering(customers_df.copy())\n",
    "    agg_stats = agg_clusterer.get_cluster_stats(clustered_agg_df, agg_clusterer.max_customers_per_cluster, agg_clusterer.max_distance_km)\n",
    "    print(\"\\nAgglomerative Clustering Stats:\")\n",
    "    for k, v in agg_stats['summary'].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"\\nSample Agglomerative Cluster Details:\")\n",
    "    for cluster_id, details in list(agg_stats.items())[:5]: # Show first 5 clusters\n",
    "        if cluster_id != 'summary':\n",
    "            print(f\"  Cluster {cluster_id}: Size={details['size']}, Diameter={details['diameter_km']:.2f} km, Size OK={details['meets_size_constraint']}, Distance OK={details['meets_distance_constraint']}\")\n",
    "\n",
    "    print(\"\\n--- Running Divisive Clustering (for comparison) ---\")\n",
    "    div_clusterer = OptimizedDivisiveGeographicClustering(\n",
    "        max_customers_per_cluster=50,\n",
    "        max_distance_km=5.0,\n",
    "        balance_clusters=True\n",
    "    )\n",
    "    clustered_div_df = div_clusterer.divisive_clustering(customers_df.copy())\n",
    "    div_stats = div_clusterer.get_cluster_stats(clustered_div_df, div_clusterer.max_customers_per_cluster, div_clusterer.max_distance_km) # Added missing arguments\n",
    "    print(\"\\nDivisive Clustering Stats:\")\n",
    "    for k, v in div_stats['summary'].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"\\nSample Divisive Cluster Details:\")\n",
    "    for cluster_id, details in list(div_stats.items())[:5]: # Show first 5 clusters\n",
    "        if cluster_id != 'summary':\n",
    "            print(f\"  Cluster {cluster_id}: Size={details['size']}, Diameter={details['diameter_km']:.2f} km, Size OK={details['meets_size_constraint']}, Distance OK={details['meets_distance_constraint']}\")\n",
    "\n",
    "    # --- Plotting the clusters (optional, requires matplotlib/folium) ---\n",
    "    # To visualize, you'd typically plot these clustered_agg_df or clustered_div_df\n",
    "    # on a map using Folium, similar to our routing visualization.\n",
    "    # For a quick visual check (requires matplotlib):\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(15, 7))\n",
    "\n",
    "        # Plot Agglomerative Clusters\n",
    "        ax1 = plt.subplot(121)\n",
    "        for cluster_id in clustered_agg_df['cluster'].unique():\n",
    "            if cluster_id == -1: continue\n",
    "            cluster_points = clustered_agg_df[clustered_agg_df['cluster'] == cluster_id]\n",
    "            ax1.scatter(cluster_points['Longitude'], cluster_points['Latitude'], \n",
    "                        label=f'Agg C{cluster_id} (n={len(cluster_points)})', s=20, alpha=0.6)\n",
    "        ax1.set_title('Agglomerative Clusters')\n",
    "        ax1.set_xlabel('Longitude')\n",
    "        ax1.set_ylabel('Latitude')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot Divisive Clusters\n",
    "        ax2 = plt.subplot(122)\n",
    "        for cluster_id in clustered_div_df['cluster'].unique():\n",
    "            if cluster_id == -1: continue\n",
    "            cluster_points = clustered_div_df[clustered_div_df['cluster'] == cluster_id]\n",
    "            ax2.scatter(cluster_points['Longitude'], cluster_points['Latitude'], \n",
    "                        label=f'Div C{cluster_id} (n={len(cluster_points)})', s=20, alpha=0.6)\n",
    "        ax2.set_title('Divisive Clusters')\n",
    "        ax2.set_xlabel('Longitude')\n",
    "        ax2.set_ylabel('Latitude')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\nMatplotlib not found. Skipping cluster visualization.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during plotting: {e}. Skipping cluster visualization.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da02ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
